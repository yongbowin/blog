## 常见问题
**短文本处理：**

**槽填充：**

**无监督的用词向量计算句向量：**

**词共现矩阵：**

**麦当劳和吃饭在一个窗口下同时出现的概率：**
发现肯德基和麦当劳，和吃饭同时出现的概率差不多，所以肯德基=麦当劳？

**实体抽取：**
词表中的实体不完全，抽取query中的实体的方法

**构建一种数据结构，实现快速查找元素**

**朴素贝叶斯算法为什么适合文本分类:**

**对不同类型的问题分别进行精调**

**字符串查找/排序/对比算法：**

**softmax推导：**

**图的反向拓扑排序：**

**最长公共子序列：**

**logistic回归推导：**

**kmeans算法：**

**计算图和链式求导：**

**写神经网络的计算图和反向传播：**

**汉诺塔的非递归实现：**

**监督学习模型的梯度下降法推导：**

**LSTM中激活函数，sigmoid**


**误差与残差区别：**
 - 误差:即观测值与真实值的偏离;
 - 残差:观测值与拟合值的偏离.
 - 误差与残差，这两个概念在某程度上具有很大的相似性，都是衡量不确定性的指标，可是两者又存在区别。 误差与测量有关，误差大小可以衡量测量的准确性，误差越大则表示测量越不准确。
 - 残差――与预测有关，残差大小可以衡量预测的准确性。残差越大表示预测越不准确。残差与数据本身的分布特性，回归方程的选择有关。

**为什么计算损失函数最优值采用梯度下降算法而不是直接求导等于0**
 - 以线性回归为例一文中详细的推导了回归理论，通过代价函数对参数求导，令其为零，得出参数为：
 
 ![line_de](img/line_de.png)
 - 参数的结果给出两个信息，同时也是直接求导不可行的原因：
    - X的转置乘以X必须要可逆，也就是**X必须可逆**，但是实际情况中并不一定都满足这个条件，因此直接求导不可行；
    - 假设满足了条件一，那么就需要去求X的转置乘以X这个整体的逆，线性代数中给出了求逆矩阵的方法，是非常复杂的(对计算机来说就是十分消耗性能的)，数据量小时，还可行，**一旦数据量大，计算机求矩阵的逆将会是一项非常艰巨的任务**，消耗的性能以及时间巨大，而在机器学习中，数据量少者上千，多者上亿；因此直接求导不可行
    - 相较而言，梯度下降算法同样能够实现最优化求解，通过多次迭代使得代价函数收敛，并且**使用梯度下降的计算成本很低**，所以基于以上两个原因，回归中多数采用梯度下降而不是求导等于零。

**如何解决sigmoid函数饱和区问题：**
 - sigmoid函数及其应用
    - sigmoid函数，它是神经网络中的一种激活函数，可以将输出限制在(0,1)范围内。
    - 在神经网络中的两种应用是：
        - a、作为神经网络中间层的一个激活函数，对于这种应用而言，sigmoid函数有两个特性是重要的，一是输出在(0,1)之间，二是非线性；
        - b、对于回归任务，在网络的最后一层将数据框定在(0,1)之间，对于这种应用而言，似乎更关注于sigmoid函数的输出在(0,1)之间这个特性，而其非线性特性就显得不那么必要；
 - sigmoid函数饱和区带来的问题
    -  所谓sigmoid函数的饱和区，是指S形中左下角和右上角的平缓区域。饱和区(平缓区域)会带来以下问题：
        - a、梯度消失问题(平缓区/饱和区梯度几乎为0)，这个问题对于sigmoid函数的两种应用都是存在的；
        - b、对于输入x，输出y的区分度不高，这个问题主要针对第二种应用；
 - 如何解决sigmoid函数饱和区问题
    - 对于a问题，一种解决方法是在sigmoid层之前引入Batch Normalization层，就是对每个神经元的输入进行规范化，即均值为0，方差为1，之后再进入激活函数。每一层规范化后，输出就以极大的概率落在靠近中心的区间，如`[-1, 1]`，这时sigmoid函数的梯度变化很大，也就解决了梯度消失的问题。
    但是这样做又有一个缺点，可以看到sigmoid函数`[-1, 1]`这段区间近似直线，也就是激活函数变成了线性的，所以整个网络绝大多数就都是线性表达，降低了神经网络的表达能力。所以BN的作者又再次引入了两个变量(γ和β)，对规范化后的神经元输出做了一次线性映射，参数也是可以学习的，
    使得最终输出落在非线性区间的概率大一些。这样就在sigmoid函数梯度小和线性表达之间做了一个平衡，使得神经网络有了非线性变换，保证了其学习能力，也使梯度比较大，加快了训练速度。

**生成一个n*n螺旋矩阵：**


**决策树、XGBoost、SVM为什么对数据不均衡样本不敏感？**


**AUC选择模型**


**CoVe/TagLM**


**深度优先算法和广度优先算法：**


**后缀数组：**


**有序数组中找出元素：**


**数字和26个字母对应，给一串数字，解码出对应的字母：**


**主动学习：**

做数据标注



**x1,x2,x3都是`[0,1]`均匀分布，三者相加还在这个区间的概率：**


**gelu激活函数：**


**MRC中为什么最后增加上NER和POS**
 - 效果好
 - 因为用的是BERT的最后一层隐藏层输出，根据ELMo的结论，低层隐藏层表征的是词法信息，高层的隐藏层表征的是语义信息，所以为了加上词法信息，加上了NER和POS。


**模型退化问题：**


**word2vec的损失函数？**


------------------------------------------------------------

**1、简要说一下Transformer的相关结构。**





**2、介绍一下Self-attention的是如何实现的**
**3、Attention的形式有哪些，为什么Attention有效果上的提升**
**4、介绍一下BERT的相关结构，BERT的优势在哪**
**5、BERT相对于ELMO和GPT的优势在哪**
**6、BERT里面，MaskLM部分是如何实现的**
**7、BERT中MaskLM 部分为什么使用随机替换**
**8、简要的介绍一下CRF，CRF是判别模型还是生成模型**
**9、简要介绍一下HMM**
**10、介绍一下维特比算法**
**11、介绍一下RNN模型**
**12、RNN为什么会出现梯度消失和梯度爆炸，怎么解决**
**13、写一下LSTM的公式，LSTM为什么能够解决梯度消息**
**14、常见的激活函数有哪些**
**15、常见的评价有哪些，都有哪些含义**
**16、如何计算AUC**
**17、介绍一下Dropout，为什么Drop能够解决过拟合，Dropout在处理训练数据和预测数据时，有哪些区别**
**18、常见的解决过拟合的方法有哪些**
**19、神经网络训练时可以将参数初始化为0吗**
**20、常见的Normalization有哪些，原理是什么**
**21、常见的池化操作有哪些，池化的作用是什么**
**22、Batch Normalization是怎么实现的，可以应用到RNN中么**
**23、请描述LSTM和GRU之间的区别是什么**
**24、介绍一下逻辑回归，逻辑回归与最大熵模型的关系**
**25、逻辑回归中为什么不能用平方差损失做损失函数**
**26、word2vec的两个模型都是怎么工作的**
**27、Fasttext的相关原理**
**28、常见的正则化有哪些，L1正则和L2正则的区别是什么**
**29、什么是过拟合和欠拟合**
**30、推到一下Linear-SVM的公式**
**31、介绍一下朴素贝叶斯模型**
**32、常见的梯度下降的方法有哪些，介绍一下Adam**
**33、介绍一下常见的损失函数**
**34、介绍一下KL散度，说一下KL散度与交叉熵的关系**
**35、介绍一下HMM和CRF的区别和联系**
**36、什么是Beam Search?它在NLP中的什么场景里会用到？它跟动态规划的区别是什么**
**37、介绍一下残差网络，原理是什么，能够解决什么问题**
**38、FT-IDF相比较Word2vec的优缺点有哪些**
**39、给定两个字符串，如何计算它们之间的相似度**

