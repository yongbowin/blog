# 信息检索

**参考文献：**
 - 《An Introduction to Neural Information Retrieval》 [pdf](https://www.microsoft.com/en-us/research/uploads/prod/2017/06/fntir2018-neuralir-mitra.pdf)
 - 《Introduction to Information Retrieval》 [pdf](https://nlp.stanford.edu/IR-book/)


## 文本检索基础

输入一个query，返回排序好的结果列表


 - 收集隐式反馈：点击决策、停留时间
 - 排序模型可以使用各种各样的输入特征：
    - 读者数量（流行程度）
    - 传入链接的数量
    - 或者根据垃圾邮件分类器判断该文章是否有问题
    - 其它的特征依赖于查询语句和文档内容的匹配
        - 传入的超链接锚点参考的文本，因为锚点和点击问题是对文档的简洁描述（不易得到）
        - 导致点击这篇文本的用户，以前的查询语句
    - 基于文档本身所含内容


短文本或者段落倾向于针对一个主题



**语义理解：**

 - 传统的方法使用query词在文档中的重复次数来建模，在问题和文本之间进行EM匹配，不同的权重和标准的模式使用这个重复次数建立了各种各样的TF-IDF模型，
 比如BM25,但是这样就忽视了剩下的相关的文本，有时候问题中的词和想要搜寻的答案之间并没有匹配的词
 - 有些时候有罕见词时，采用直接匹配的效果可能胜过语义查询的效果，比如词“魑魅魍魉”
 - 因此如何权衡语义理解进而查询和直接匹配查询是关键点


**对罕见输入的鲁棒性：**

70%的问题仅仅出现一次，50%的文档仅仅被点击过一次

 - BM25算法可以很好地检索包含罕见词的文档
 

**对语料方差的鲁棒性**





## 主题建模

参考博客：[一文读懂如何用LSA、PSLA、LDA和lda2vec进行主题建模](https://www.zhihu.com/search?type=content&q=LSA)

**一般概念：**
 - 在文档层面，理解文本最有效的方式之一就是分析其主题
 - 在文档集合中学习、识别和提取这些主题的过程被称为主题建模
 - 主要技术有LSA、pLSA、LDA、lda2vec
 - 所有主题模型都基于相同的基本假设：
    - 每个文档包含多个主题；
    - 每个主题包含多个单词。
 - 主题模型围绕着以下观点构建：
    - 实际上，文档的语义由一些我们所忽视的隐变量或「潜」变量管理。
    - 因此，主题建模的目标就是揭示这些潜在变量（也就是主题），正是它们塑造了我们文档和语料库的含义。

#### 潜在语义分析（LSA）
**一般概念：**
 - 潜在语义分析（LSA）是主题建模的基础技术之一。
 - 其核心思想是把我们所拥有的文档-术语矩阵分解成相互独立的**文档-主题矩阵**和**主题-术语矩阵**。

**LSA:**
 - 第一步是生成文档-术语矩阵。可以构造一个`m×n`的矩阵A，其中每行代表一个文档，每列代表一个单词。
 - 在LSA的最简单版本中，每一个条目可以简单地是第j个单词在第i个文档中出现次数的原始计数。
     - 然而，原始计数的效果不是很好，无法考虑文档中每个词的权重。
     - 因此，LSA模型通常用tf-idf得分代替文档-术语矩阵中的原始计数。为文档i中的术语j分配了相应的权重
     - 术语出现在文档中的频率越高，同时，术语在语料库中出现的频率越低，其权重越大。
 - 一旦拥有文档-术语矩阵A，我们就可以开始思考潜在主题，但是，A极有可能非常稀疏、噪声很大，并且在很多维度上非常冗余。因此，为了找出能够捕捉单词和文档关系的少数潜在主题，我们希望能降低矩阵A的维度。
 - 这种降维可以使用截断SVD（即奇异值分解）来执行。
     ![svd](img/svd.png)

     ```
     该技术将任意矩阵M分解为三个独立矩阵的乘积：
    
     `M=U*S*V`，
    
     其中S是矩阵M奇异值的对角矩阵。
     截断SVD的降维方式是：
         - 选择奇异值中最大的t个数，且只保留矩阵U和V的前t列。
         - 在这种情况下，t是一个超参数，我们可以根据想要查找的主题数量进行选择和调整。
    
     A = U_t * S_t * V_{t}^T
     截断SVD可以看作只保留我们变换空间中最重要的t维。
    
     A = (文档-主题矩阵) * S_t * (术语-主题矩阵)^T
    
     在这种情况下:
         - U ∈ ℝ^(m ⨉ t) 是文档-主题矩阵
         - V ∈ ℝ^(n ⨉ t) 是术语-主题矩阵，进行了转置
         - 在矩阵U和V中，每一列对应于我们t个主题当中的一个
         - 在U中，行表示按主题表达的文档向量；在V中，行代表按主题表达的术语向量。
     ```
 - 通过这些文档向量和术语向量，现在可以应用余弦相似度等度量来评估以下指标：
    - 不同文档的相似度
    - 不同单词的相似度
    - **术语（或「queries」）与文档的相似度**（当我们想要检索与查询最相关的段落，即进行信息检索时，这一点将非常有用）
 - LSA优缺点：
    - 优点：快速且高效
    - 缺点：
        - 缺乏可解释的嵌入（我们并不知道主题是什么，其成分可能积极或消极，这一点是随机的）
        - 需要大量的文件和词汇来获得准确的结果
        - 表征效率低

**代码实现：**
```
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import TruncatedSVD
from sklearn.pipeline import Pipeline
documents = ["doc1.txt", "doc2.txt", "doc3.txt"] 

# raw documents to tf-idf matrix: 
vectorizer = TfidfVectorizer(stop_words='english', 
                             use_idf=True, 
                             smooth_idf=True)
# SVD to reduce dimensionality: 
svd_model = TruncatedSVD(n_components=100,         // num dimensions
                         algorithm='randomized',
                         n_iter=10)
# pipeline of tf-idf + SVD, fit to and applied to documents:
svd_transformer = Pipeline([('tfidf', vectorizer), 
                            ('svd', svd_model)])
svd_matrix = svd_transformer.fit_transform(documents)

# svd_matrix can later be used to compare documents, compare words, or compare queries with documents
```


#### 概率潜在语义分析（PLSA）
**一般概念：**
 - pLSA，即概率潜在语义分析，**采取概率方法替代SVD**以解决问题
 - 其核心思想是**找到一个潜在主题的概率模型**，该模型可以生成我们在文档-术语矩阵中观察到的数据。
 - 特别地，我们需要一个模型`P(D,W)`，使得对于任何文档d和单词w，`P(d,w)`能对应于文档-术语矩阵中的那个条目。
 - 主题模型的基本假设：每个文档由多个主题组成，每个主题由多个单词组成。PLSA为这些假设增加了概率：
    - 给定文档d，主题z以`P(z|d)`的概率出现在该文档d中
    - 给定主题z，单词w以`P(w|z)`的概率从主题z中提取出来    
    ![plsa](img/plsa.png)
    - 从形式上看，一个给定的文档和单词同时出现的联合概率是：
        ```
        P(D,W) = P(D) sum_{Z} P(Z|D) P(W|Z)
        ```
        - 等式右边告诉我们理解某个文档的可能性有多大；然后根据该文档主题的分布情况，在该文档中找到某个单词的可能性有多大
        - 在这种情况下，`P(D)`、`P(Z|D)`、和`P(W|Z)`是我们模型的参数。`P(D)`可以直接由我们的语料库确定。`P(Z|D)`和`P(W|Z)`利用了多项式分布建模，
        并且可以使用期望最大化算法（EM）进行训练。EM无需进行算法的完整数学处理，而是一种基于未观测潜变量（此处指主题）的模型找到最可能的参数估值的方法。
        - `P(D,W)`可以利用不同的的3个参数等效地进行参数化：
            ```
            P(D,W) = sum_{Z} P(Z) P(D|Z) P(W|Z)
            ```
            ![params_plsa](img/params_plsa.png)
            - 可以通过将模型看作一个生成过程来理解这种等价性。
            - 在第一个参数化过程中，我们从概率为`P(d)`的文档开始，然后用`P(z|d)`生成主题，最后用`P(w|z)`生成单词。而在上述这个参数化过程中，
            我们从`P(z)`开始，再用`P(d|z)`和`P(w|z)`单独生成文档。
            - 可以发现pLSA模型和LSA模型之间存在一个直接的平行对应关系
            ![plsa_lsa](img/plsa_lsa.png)
            主题`P(Z)`的概率对应于奇异主题概率的对角矩阵，给定主题`P(D|Z)`的文档概率对应于文档-主题矩阵U，给定主题`P(W|Z)`的单词概率对应于术语-主题矩阵V
 - 尽管pLSA看起来与LSA差异很大、且处理问题的方法完全不同，但实际上pLSA只是在LSA的基础上**添加了对主题和词汇的概率处理**罢了。
 - pLSA是一个更加灵活的模型，但仍然存在一些问题，尤其表现为：
    - 因为我们没有参数来给`P(D)`建模，所以不知道如何为新文档分配概率
    - pLSA的参数数量随着我们拥有的文档数线性增长，因此容易出现过度拟合问题
    - 很少会单独使用pLSA。一般来说，当人们在寻找超出LSA基准性能的主题模型时，他们会转而使用LDA模型。LDA是最常见的主题模型，它在pLSA的基础上进行了扩展，从而解决这些问题。


#### 潜在狄利克雷分布（LDA）

参考博客：
 - [LDA主题模型笔记](https://www.cnblogs.com/yang901112/p/11644930.html)
 - [主题模型-LDA浅析](https://blog.csdn.net/huagong_adu/article/details/7937616)
 - [LDA主题模型详解](https://blog.csdn.net/daycym/article/details/88876460)

**一般概念：**
 - LDA即潜在狄利克雷分布，是pLSA的贝叶斯版本
 - 它使用狄利克雷先验来处理文档-主题和单词-主题分布，从而有助于更好地泛化。
 - 数学概念：
    - Beta分布是二项分布的共轭先验概率分布
    - Dirichlet分布是多项式分布的共轭先验概率分布
    - 先验分布π(θ) + 样本信息x ⇒ 后验分布 π(θ∣x)
    
        ```
        上述思考模式意味着，新观察到的样本信息将修正人们以前对事物的认知。换言之，在得到新的样本信息之前，人们对θ的认知是先验分布π(θ)，在得到新的样本信息X后，人们对的认知为π(θ∣x)
        ```

**LDA模型：**
 - 应用于信息提取和搜索(语义分析)；文档分类/聚类、文章摘要、社区挖掘；基于内容的图像聚类、目标识别(以及其他计算机视觉应用)；生物信息数据的应用;
 - 对于朴素贝叶斯模型来说，可以胜任许多文本分类问题，但无法解决语料中一词多义和多词一义的问题--它更像是词法分析，而非语义分析。如果使用词向量作为文档的特征，一词多义和多词一义会造成计算文档间相似度的不准确性。LDA模型通过增加“主题”的方式，一定程度的解决上述问题：
 - **一个词可能被映射到多个主题中，即，一词多义。多个词可能被映射到某个主题的概率很高，即，多词一义**。
 - 在贝叶斯概率理论中，如果后验概率P(θ|x)和先验概率p(θ)满足同样的分布律，那么，先验分布和后验分布被叫做共轭分布，同时，先验分布叫做似然函数的共轭先验分布。
    ![gexy](img/gexy.png)
 - 投币过程中，正面朝上的次数，α和β先验性的给出了在没有任何实验的前提下，硬币朝上的概率分配；因此，α和β可被称作“伪计数”。
 - 从2到K：二项分布→多项分布，Beta分布→Dirichlet分布
 - 做LDA的时候，在条件允许的情况下，α值尽量不要设置太大，这样做的好处是充分考虑样本的因素，而不要过分考虑先验参数的影响。
 - 当然，如果先验给的大，就是更多考虑先验，而不是样本。这需要充分根据实际情况决定，如果说我们认为样本的情况是重要的，就不要加入太大的先验。
 - LDA的解释：
    - 共有m篇文章，一共涉及了K个主题
    - 每篇文章都有各自的主题分布，主题分布是多项分布，该多项分布的参数服从Dirichlet分布，该Dirichlet分布的参数为α
    - 每个主题都有各自的词分布，词分布为多项分布，该多项分布的参数服从Dirichlet分布，该Dirichlet分布的参数为β
    - 对于某篇文章中的第n个词，首先从该文章的主题分布中采样一个主题，然后在这个主题对应的词分布中采样一个词。不断重复这个随机生成过程，直到m篇文章全部完成上述过程。
    - 详细解释：此段非常有利于理解LDA主题模型：
        ![lda_detail](img/lda_detail.png)
 - LDA总结：
    - 由于在词和文档之间加入的主题的概念，可以较好的解决一词多义和多词一义的问题。
    - 在实践中发现，LDA用于短文档往往效果不明显，因为一个词被分配给某个主题的次数和一个主题包括的词的数目尚未收敛，往往需要其他方案连接成长文档
    - LDA可以和其他算法相结合。首先使用LDA将长度为Ni的文档降维到K维(主题的数目)（参见SVD，LSA奇异值分解），同时给出每个主题的概率(主题分布)，从而可以使用TF-IDF继续分析或者直接作为文档的特征进入聚类或者标签传播算法——用于社区发现等问题
 - ----------------------------- 分割线 -----------------------------
 - 传统判断两个文档相似性的方法是通过查看两个文档共同出现的单词的多少，如TF-IDF等，这种方法没有考虑到文字背后的语义关联，可能在两个文档共同出现的单词很少甚至没有，但两个文档是相似的。
 在判断文档相关性的时候需要考虑到文档的语义，而语义挖掘的利器是主题模型，LDA就是其中一种比较有效的模型
 - 可以用生成模型来看文档和主题这两件事。所谓生成模型，就是说，我们认为一篇文章的每个词都是通过**以一定概率选择了某个主题，并从这个主题中以一定概率选择某个词语**这样一个过程得到的
    - 其中”文档-词语”矩阵表示每个文档中每个单词的词频，即出现的概率；
    - ”主题-词语”矩阵表示每个主题中每个单词的出现概率；
    - ”文档-主题”矩阵表示每个文档中每个主题出现的概率。
 - 主题是被抽象出来的介于文档和单词之间的一种概念
 - 主题模型(Topic Model)是用来在一系列文档中发现抽象主题的一种统计模型。
 - 主题模型 就是一种自动分析每个文档，统计文档中的词语，根据统计的信息判断当前文档包含哪些主题以及各个主题所占比例各为多少
 - LDA 主题模型要干的事就是：根据给定的一篇文档，反推其主题分布。
 - 如何生成M个包含N个单词的文档？（3种方式）
    - 基础模型1：`Unigram model`
    
        ```
        For each of the N words w_n: 
                Choose a word w_n ～ p(w); //挑选单词符合分布p(w)
        
        其中：
            - N表示要生成的文档的单词的个数，
            - w_n表示生成的第n个单词w，
            - p(w)表示单词w的分布，可以通过语料进行统计学习得到
        
        这种方法通过训练语料获得一个单词的概率分布函数，然后根据这个概率分布函数每次生成一个单词，使用这个方法M次生成M个文档
        ```
    - 基础模型2：`Mixture of unigram`
        - unigram模型的方法的缺点就是生成的文本没有主题，过于简单，mixture of unigram方法对其进行了改进，该模型使用下面方法生成1个文档
        
            ```
            Choose a topic z ～ p(z); //从主题分布中选择一个主题
            For each of the N words w_n: 
                    Choose a word w_n ～ p(w|z); 
            
            其中：
                - z表示一个主题，p(z)表示主题的概率分布，z通过p(z)按概率产生；
                - N和w_n同上；
                - p(w|z)表示给定z时w的分布
            
            可以看成一个k×V的矩阵，k为主题的个数，V为单词的个数，每行表示这个主题对应的单词的概率分布，即主题z所包含的各个单词的概率，
            通过这个概率分布按一定概率生成每个单词。
            ```
        - 这种方法首先选选定一个主题z，主题z对应一个单词的概率分布`p(w|z)`，每次按这个分布生成一个单词，使用M次这个方法生成M份不同的文档
        - 该方式只允许一个文档只有一个主题，这不太符合常规情况，通常一个文档可能包含多个主题
    - 基础模型3：`LDA(Latent Dirichlet Allocation)`
        - LDA方法使生成的文档可以包含多个主题，该模型使用下面方法生成1个文档：
        
            ```
            Choose parameter θ ～ p(θ);
                        For each of the N words w_n:
                                Choose a topic z_n ～ p(z|θ);
                                Choose a word w_n ～ p(w|z);
            
            其中θ是一个主题向量，向量的每一列表示每个主题在文档出现的概率，该向量为非负归一化向量
                - p(θ)是θ的分布，具体为Dirichlet分布，即分布的分布；
                - N和w_n同上；
                - z_n表示选择的主题，
                - p(z|θ)表示给定θ时主题z的概率分布，具体为θ的值，即p(z=i|θ)= θ_i；
                - p(w|z)同上
            
            过程如下：
                - 这种方法首先选定一个主题向量θ，确定每个主题被选择的概率。
                - 然后在生成每个单词的时候，从主题分布向量θ中选择一个主题z，按主题z的单词概率分布生成一个单词。
            ```
 - ----------------------------- 分割线 -----------------------------
 - 在LDA主题模型中，一篇文档生成的方式如下：
 
    ![lda_doc](img/lda_doc.png)
 - 结构图：
 
    ![lda_arch](img/lda_arch.png)
