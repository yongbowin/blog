# 信息检索

## 文本检索基础

输入一个query，返回排序好的结果列表


 - 收集隐式反馈：点击决策、停留时间
 - 排序模型可以使用各种各样的输入特征：
    - 读者数量（流行程度）
    - 传入链接的数量
    - 或者根据垃圾邮件分类器判断该文章是否有问题
    - 其它的特征依赖于查询语句和文档内容的匹配
        - 传入的超链接锚点参考的文本，因为锚点和点击问题是对文档的简洁描述（不易得到）
        - 导致点击这篇文本的用户，以前的查询语句
    - 基于文档本身所含内容


短文本或者段落倾向于针对一个主题



**语义理解：**

 - 传统的方法使用query词在文档中的重复次数来建模，在问题和文本之间进行EM匹配，不同的权重和标准的模式使用这个重复次数建立了各种各样的TF-IDF模型，
 比如BM25,但是这样就忽视了剩下的相关的文本，有时候问题中的词和想要搜寻的答案之间并没有匹配的词
 - 有些时候有罕见词时，采用直接匹配的效果可能胜过语义查询的效果，比如词“魑魅魍魉”
 - 因此如何权衡语义理解进而查询和直接匹配查询是关键点


**对罕见输入的鲁棒性：**

70%的问题仅仅出现一次，50%的文档仅仅被点击过一次

 - BM25算法可以很好地检索包含罕见词的文档
 

**对语料方差的鲁棒性**




## 传统方法

**BM25：**
 - 公式如下：
 
    ![bm25](img/bm25.png)
 
 - 形式化表示：
 
    ```
    对query中的每一个词t_q计算：
    
                                                         (query中的词t_q在文档d中的频率) · (k1 + 1)
    v_{t_q} = (query中词t_q的逆文档频率) · -------------------------------------------------------------------------
                                        (query中的词t_q在文档d中的频率) + k1 · (1 - b + b · (文档d的长度 / 平均文档长度))
        
    然后，将每一个词得到的结果加和，得到BM25的值。
    
    默认值设置：
     - k1取值在[1.2, 2.0]
     - b = 0.75
    ```
 - 逆文档频率的计算：
 
    ![bm25_1](img/bm25_1.png)
 - 形式化表示：
 
    ```
                       (文档总数量) - (包含词t的文档数) + 0.5
    词t的逆文档频率 = log ----------------------------------
                              (包含词t的文档数) + 0.5
    ```
 - BM25算法聚合了query中不同词的贡献，但是忽视了词组和不同的查询词在文档中出现的临近的信号

**基于语言模型的排序：**
 - language modelling
 - 基于语言模型的方法，利用后验概率`p(d|q)`来排序文档
 
    ![ir_lm](img/ir_lm.png)
 - `p(q|d)`表示生成query q的概率，通过最大似然估计来进行估计
 
    ![ir_lm_1](img/ir_lm_1.png)
 - 两种常用的平滑方法：
 
    ![ir_lm_2](img/ir_lm_2.png)
 - TF-IDF、BM25、语言模型都是基于query词在文档中的数量来估计与文档相关性，它们出现的位置信息和在文档中与其他词的关系被忽略了。

**基于翻译模型的排序：**
 - Translation model
 - 即假设query q是文档d通过翻译过程生成的，以此来估计`p(t_q|d)`
 
    ![ir_tm](img/ir_tm.png)
 - 其中`p(t_q|t_d)`组件允许模型从文档中的non-query词去收集相关性的证据

**基于依赖模型的排序：**
 - Dependence model
 - 基于一个线性模型，考虑临近的特征
 
    ![ir_dm](img/ir_dm.png)

**PRF：**
 - Pseudo relevance feedback (PRF)，例如Relevance Models (RM)，通过执行额外的一轮检索来展示强大的性能
 - 第一轮检索中的排序文档集合R1用于选择query的扩展项以扩展查询，扩展后的query再次进行检索新的排序文档集合R2。
 - 在RM中通过计算query语言模型`theta_q`和文档语言模型`theta_d`的KL散度给文档进行打分：
 
    ![ir_rm](img/ir_rm.png)
    
    ![ir_rm_1](img/ir_rm_1.png)
   
 其中，T是词汇表。
 - 在RM3模型下，新的query语言模型计算方式如下：
 
    ![ir_rm_2](img/ir_rm_2.png)

**总结：**
 - TF-IDF和语言模型仅仅考虑query词在文档中的出现频率
 - Dependence模型考虑吧短语的匹配
 - Translation模型和PRF模型可以处理query和文档的词汇不匹配的问题




















## 词表示的无监督学习



























































































## 主题建模
**一般概念：**
 - 主题模型是一个生成模型
 - 在文档层面，理解文本最有效的方式之一就是分析其主题
 - 在文档集合中学习、识别和提取这些主题的过程被称为主题建模
 - 主题模型也是非监督的算法
 - 主要技术有LSA、pLSA、LDA、lda2vec
 - 所有主题模型都基于相同的基本假设：
    - 每个文档包含多个主题；
    - 每个主题包含多个单词。
 - 主题模型围绕着以下观点构建：
    - 实际上，文档的语义由一些我们所忽视的隐变量或「潜」变量管理。
    - 因此，主题建模的目标就是揭示这些潜在变量（也就是主题），正是它们塑造了我们文档和语料库的含义。

#### 潜在语义分析（LSA/LSI）

参考博客：
 - [文本主题模型之潜在语义索引(LSI)](https://www.cnblogs.com/pinard/p/6805861.html)

**一般概念：**
 - 潜在语义分析（LSA）是主题建模的基础技术之一。
 - LSA/LSI是基于奇异值分解（SVD）的方法来得到文本的主题的。
 - 其核心思想是把我们所拥有的文档-术语矩阵分解成相互独立的**文档-主题矩阵**和**主题-术语矩阵**。
 - 主题模型也是非监督的算法，目的是得到文本按照主题的概率分布
 - LDA和普通聚类算法的区别：
    - 聚类算法关注于从样本特征的相似度方面将数据聚类。比如通过数据样本之间的欧式距离，曼哈顿距离的大小聚类等。
    - 而主题模型，顾名思义，就是对文字中隐含主题的一种建模方法。比如从“人民的名义”和“达康书记”这两个词我们很容易发现对应的文本有很大的主题相关度，但是如果通过词特征来聚类的话则很难找出，因为聚类方法不能考虑到到隐含的主题这一块。
 - 那么如何找到隐含的主题呢？
    - 常用的方法一般都是基于统计学的生成方法。即假设以一定的概率选择了一个主题，然后以一定的概率选择当前主题的词。最后这些词组成了我们当前的文本。
    - 所有词的统计概率分布可以从语料库获得，具体如何以“一定的概率选择”，这就是各种具体的主题模型算法的任务了。
　　　- 当然还有一些不是基于统计的方法，比如LSI
 - 潜在语义索引(Latent Semantic Indexing, LSI)，有的也叫Latent Semantic Analysis (LSA)

**LSA:**
 - 第一步是生成文档-术语矩阵。可以构造一个`m×n`的矩阵A，其中每行代表一个文档，每列代表一个单词。
 - 在LSA的最简单版本中，每一个条目可以简单地是第j个单词在第i个文档中出现次数的原始计数。
     - 然而，原始计数的效果不是很好，无法考虑文档中每个词的权重。
     - 因此，LSA模型通常用tf-idf得分代替文档-术语矩阵中的原始计数。为文档i中的术语j分配了相应的权重
     - 术语出现在文档中的频率越高，同时，术语在语料库中出现的频率越低，其权重越大。
 - 一旦拥有文档-术语矩阵A，我们就可以开始思考潜在主题，但是，A极有可能非常稀疏、噪声很大，并且在很多维度上非常冗余。因此，为了找出能够捕捉单词和文档关系的少数潜在主题，我们希望能降低矩阵A的维度。
 - 这种降维可以使用截断SVD（即奇异值分解）来执行。
     
     ![svd](img/svd.png)

     ```
     该技术将任意矩阵M分解为三个独立矩阵的乘积：
    
     `M=U*S*V`，
    
     其中S是矩阵M奇异值的对角矩阵。
     截断SVD的降维方式是：
         - 选择奇异值中最大的t个数，且只保留矩阵U和V的前t列。
         - 在这种情况下，t是一个超参数，我们可以根据想要查找的主题数量进行选择和调整。
    
     A = U_t * S_t * V_{t}^T
     截断SVD可以看作只保留我们变换空间中最重要的t维。
    
     A = (文档-主题矩阵) * S_t * (术语-主题矩阵)^T
    
     在这种情况下:
         - U ∈ ℝ^(m ⨉ t) 是文档-主题矩阵
         - V ∈ ℝ^(n ⨉ t) 是术语-主题矩阵，进行了转置
         - 在矩阵U和V中，每一列对应于我们t个主题当中的一个
         - 在U中，行表示按主题表达的文档向量；在V中，行代表按主题表达的术语向量。
     ```
 - 而`A_{ij}`则对应第i个文本的第j个词的特征值，这里最常用的是**基于预处理后的标准化TF-IDF值**。
 - 通过这些文档向量和术语向量，现在可以应用余弦相似度等度量来评估以下指标：
    - 不同文档的相似度
    - 不同单词的相似度
    - **术语（或「queries」）与文档的相似度**（当我们想要检索与查询最相关的段落，即进行信息检索时，这一点将非常有用）
 - LSA优缺点：
    - 优点：快速且高效
    - 缺点：
        - 缺乏可解释的嵌入（我们并不知道主题是什么，其成分可能积极或消极，这一点是随机的）
        - 需要大量的文件和词汇来获得准确的结果
        - 表征效率低
 - ------------------------------- 分割线 -------------------------------
 - 简单的LSI实例，假设我们有下面这个有11个词三个文本的词频TF对应矩阵如下：
 
     ![lsa_form](img/lsa_form.png)
    
    - 这里我们没有使用预处理，也没有使用TF-IDF，在实际应用中最好使用预处理后的TF-IDF值矩阵作为输入
    - 假定对应的主题数为2，则通过SVD降维后得到的三矩阵为:
    
        ![svd_sample](img/svd_sample.png)
        
        其中，`U_k`表示主题-术语矩阵，`V_k`表示主题-文档矩阵。
    - 可以看到里面有负数，所以这样得到的相关度比较难解释。需要把文档-术语的词频矩阵替换掉，使用TF-IDF来代替。
    - 在上面我们通过LSI得到的**文档-主题矩阵可以用于文本相似度计算**
    
        ```
        而计算方法一般是通过余弦相似度。比如对于上面的三文档两主题的例子。我们可以计算第一个文本和第二个文本的余弦相似度如下:
        ```
        
        ![similar_comp](img/similar_comp.png)
 - LSA/LSI总结：
    - LSI是最早出现的主题模型了，它的算法原理很简单，一次奇异值分解就可以得到主题模型，同时解决词义的问题，非常漂亮。但是LSI有很多不足，导致它在当前实际的主题模型中已基本不再使用。
    - 主要的问题有：
        - 1）SVD计算非常的耗时，尤其是文本处理，词和文本数都是非常大的，对于这样的高维度矩阵做奇异值分解是非常难的。
        - 2）主题值的选取对结果的影响非常大，很难选择合适的k值。
        - 3）LSI得到的不是一个概率模型，缺乏统计基础，结果难以直观的解释。
    - 解决方案：
        - 对于问题1），主题模型非负矩阵分解（NMF）可以解决矩阵分解的速度问题。
        - 对于问题2），这是老大难了，大部分主题模型的主题的个数选取一般都是凭经验的，较新的层次狄利克雷过程（HDP）可以自动选择主题个数。
        - 对于问题3），pLSI(也叫pLSA)和隐含狄利克雷分布(LDA)这类基于概率分布的主题模型来替代基于矩阵分解的主题模型。
    - 回到LSI本身，对于一些规模较小的问题，如果想快速粗粒度的找出一些主题分布的关系，则LSI是比较好的一个选择，其他时候，如果你需要使用主题模型，推荐使用LDA和HDP


**代码实现：**
```
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import TruncatedSVD
from sklearn.pipeline import Pipeline
documents = ["doc1.txt", "doc2.txt", "doc3.txt"] 

# raw documents to tf-idf matrix: 
vectorizer = TfidfVectorizer(stop_words='english', 
                             use_idf=True, 
                             smooth_idf=True)
# SVD to reduce dimensionality: 
svd_model = TruncatedSVD(n_components=100,         // num dimensions
                         algorithm='randomized',
                         n_iter=10)
# pipeline of tf-idf + SVD, fit to and applied to documents:
svd_transformer = Pipeline([('tfidf', vectorizer), 
                            ('svd', svd_model)])
svd_matrix = svd_transformer.fit_transform(documents)

# svd_matrix can later be used to compare documents, compare words, or compare queries with documents
```


#### 概率潜在语义分析（PLSA/PLSI）
**一般概念：**
 - pLSA，即概率潜在语义分析，**采取概率方法替代SVD**以解决问题
 - 其核心思想是**找到一个潜在主题的概率模型**，该模型可以生成我们在文档-术语矩阵中观察到的数据。
 - 特别地，我们需要一个模型`P(D,W)`，使得对于任何文档d和单词w，`P(d,w)`能对应于文档-术语矩阵中的那个条目。
 - 主题模型的基本假设：每个文档由多个主题组成，每个主题由多个单词组成。PLSA为这些假设增加了概率：
    - 给定文档d，主题z以`P(z|d)`的概率出现在该文档d中
    - 给定主题z，单词w以`P(w|z)`的概率从主题z中提取出来    
    
    ![plsa](img/plsa.png)
    - 从形式上看，一个给定的文档和单词同时出现的联合概率是：
        ```
        P(D,W) = P(D) sum_{Z} P(Z|D) P(W|Z)
        ```
        - 等式右边告诉我们理解某个文档的可能性有多大；然后根据该文档主题的分布情况，在该文档中找到某个单词的可能性有多大
        - 在这种情况下，`P(D)`、`P(Z|D)`、和`P(W|Z)`是我们模型的参数。`P(D)`可以直接由我们的语料库确定。`P(Z|D)`和`P(W|Z)`利用了多项式分布建模，
        并且可以使用期望最大化算法（EM）进行训练。EM无需进行算法的完整数学处理，而是一种基于未观测潜变量（此处指主题）的模型找到最可能的参数估值的方法。
        - `P(D,W)`可以利用不同的的3个参数等效地进行参数化：
            ```
            P(D,W) = sum_{Z} P(Z) P(D|Z) P(W|Z)
            ```
            
            ![params_plsa](img/params_plsa.png)
            - 可以通过将模型看作一个生成过程来理解这种等价性。
            - 在第一个参数化过程中，我们从概率为`P(d)`的文档开始，然后用`P(z|d)`生成主题，最后用`P(w|z)`生成单词。而在上述这个参数化过程中，
            我们从`P(z)`开始，再用`P(d|z)`和`P(w|z)`单独生成文档。
            - 可以发现pLSA模型和LSA模型之间存在一个直接的平行对应关系
            
            ![plsa_lsa](img/plsa_lsa.png)
            
            主题`P(Z)`的概率对应于奇异主题概率的对角矩阵，给定主题`P(D|Z)`的文档概率对应于文档-主题矩阵U，给定主题`P(W|Z)`的单词概率对应于术语-主题矩阵V
 - 尽管pLSA看起来与LSA差异很大、且处理问题的方法完全不同，但实际上pLSA只是在LSA的基础上**添加了对主题和词汇的概率处理**罢了。
 - pLSA是一个更加灵活的模型，但仍然存在一些问题，尤其表现为：
    - 因为我们没有参数来给`P(D)`建模，所以不知道如何为新文档分配概率
    - pLSA的参数数量随着我们拥有的文档数线性增长，因此容易出现过度拟合问题
    - 很少会单独使用pLSA。一般来说，当人们在寻找超出LSA基准性能的主题模型时，他们会转而使用LDA模型。LDA是最常见的主题模型，它在pLSA的基础上进行了扩展，从而解决这些问题。
 - ----------------------------- 分割线 -----------------------------
 - 假设在一篇文档d中，主题用c来表示，词用w来表示，则有如下公式：
 
    ![plsa_form](img/plsa_form.png)
 
 解释如下：
    - 第一个等式是对称形式，其主要思路是认为文档和词都按照一定的概率分布从主题c中产生；
    - 第二个等式是非对称形式，更符合我们的直觉，主要思路是从该文档中按照一定概率分布选择一个主题，然后再按照一定概率从该主题中选择这个词。
 - 把这里的非对称形式的公式左右都除以`P(d)`便得到下面这个公式：
 
    ![plsa_form_1](img/plsa_form_1.png)
 
 式子中的`P(w,d)`除以`P(d)`得到`P(w|d)`。
 - 总结：
    - pLSA的参数个数是`cd+wc`，所以参数个数随着文档d的增加而线性增加。
    - 但是很重要的的是，**pLSA只是对已有文档的建模，也就是说生成模型只适合于这些用以训练pLSA算法的文档，并不是新文档的生成模型**。这一点很重要，因为我们后文要说的**pLSA很容易过拟合，还有LDA为了解决这些问题引入的狄利克雷分布**都与此有关。


#### 潜在狄利克雷分布（LDA）
**一般概念：**
 - LDA即潜在狄利克雷分布，是pLSA的贝叶斯版本
 - 它使用狄利克雷先验来处理文档-主题和单词-主题分布，从而有助于更好地泛化。
 - 数学概念：
    - Beta分布是二项分布的共轭先验概率分布
    - Dirichlet分布是多项式分布的共轭先验概率分布
    - 先验分布π(θ) + 样本信息x ⇒ 后验分布 π(θ∣x)
    
        ```
        上述思考模式意味着，新观察到的样本信息将修正人们以前对事物的认知。换言之，在得到新的样本信息之前，人们对θ的认知是先验分布π(θ)，在得到新的样本信息X后，人们对的认知为π(θ∣x)
        ```

**LDA模型：**
 - 应用于信息提取和搜索(语义分析)；文档分类/聚类、文章摘要、社区挖掘；基于内容的图像聚类、目标识别(以及其他计算机视觉应用)；生物信息数据的应用;
 - 对于朴素贝叶斯模型来说，可以胜任许多文本分类问题，但无法解决语料中一词多义和多词一义的问题--它更像是词法分析，而非语义分析。如果使用词向量作为文档的特征，一词多义和多词一义会造成计算文档间相似度的不准确性。LDA模型通过增加“主题”的方式，一定程度的解决上述问题：
 - **一个词可能被映射到多个主题中，即，一词多义。多个词可能被映射到某个主题的概率很高，即，多词一义**。
 - 在贝叶斯概率理论中，如果后验概率P(θ|x)和先验概率p(θ)满足同样的分布律，那么，先验分布和后验分布被叫做共轭分布，同时，先验分布叫做似然函数的共轭先验分布。
    
    ![gexy](img/gexy.png)
 - 投币过程中，正面朝上的次数，α和β先验性的给出了在没有任何实验的前提下，硬币朝上的概率分配；因此，α和β可被称作“伪计数”。
 - 从2到K：二项分布→多项分布，Beta分布→Dirichlet分布
 - 做LDA的时候，在条件允许的情况下，α值尽量不要设置太大，这样做的好处是充分考虑样本的因素，而不要过分考虑先验参数的影响。
 - 当然，如果先验给的大，就是更多考虑先验，而不是样本。这需要充分根据实际情况决定，如果说我们认为样本的情况是重要的，就不要加入太大的先验。
 - LDA的解释：
    - 共有m篇文章，一共涉及了K个主题
    - 每篇文章都有各自的主题分布，主题分布是多项分布，该多项分布的参数服从Dirichlet分布，该Dirichlet分布的参数为α
    - 每个主题都有各自的词分布，词分布为多项分布，该多项分布的参数服从Dirichlet分布，该Dirichlet分布的参数为β
    - 对于某篇文章中的第n个词，首先从该文章的主题分布中采样一个主题，然后在这个主题对应的词分布中采样一个词。不断重复这个随机生成过程，直到m篇文章全部完成上述过程。
    - 详细解释：此段非常有利于理解LDA主题模型：
        
        ![lda_detail](img/lda_detail.png)
 - LDA总结：
    - 由于在词和文档之间加入的主题的概念，可以较好的解决一词多义和多词一义的问题。
    - 在实践中发现，LDA用于短文档往往效果不明显，因为一个词被分配给某个主题的次数和一个主题包括的词的数目尚未收敛，往往需要其他方案连接成长文档
    - LDA可以和其他算法相结合。首先使用LDA将长度为Ni的文档降维到K维(主题的数目)（参见SVD，LSA奇异值分解），同时给出每个主题的概率(主题分布)，从而可以使用TF-IDF继续分析或者直接作为文档的特征进入聚类或者标签传播算法——用于社区发现等问题
 - ----------------------------- 分割线 -----------------------------
 - 传统判断两个文档相似性的方法是通过查看两个文档共同出现的单词的多少，如TF-IDF等，这种方法没有考虑到文字背后的语义关联，可能在两个文档共同出现的单词很少甚至没有，但两个文档是相似的。
 在判断文档相关性的时候需要考虑到文档的语义，而语义挖掘的利器是主题模型，LDA就是其中一种比较有效的模型
 - 可以用生成模型来看文档和主题这两件事。所谓生成模型，就是说，我们认为一篇文章的每个词都是通过**以一定概率选择了某个主题，并从这个主题中以一定概率选择某个词语**这样一个过程得到的
    - 其中”文档-词语”矩阵表示每个文档中每个单词的词频，即出现的概率；
    - ”主题-词语”矩阵表示每个主题中每个单词的出现概率；
    - ”文档-主题”矩阵表示每个文档中每个主题出现的概率。
 - 主题是被抽象出来的介于文档和单词之间的一种概念
 - 主题模型(Topic Model)是用来在一系列文档中发现抽象主题的一种统计模型。
 - 主题模型 就是一种自动分析每个文档，统计文档中的词语，根据统计的信息判断当前文档包含哪些主题以及各个主题所占比例各为多少
 - LDA 主题模型要干的事就是：根据给定的一篇文档，反推其主题分布。
 - 如何生成M个包含N个单词的文档？（3种方式）
    - 基础模型1：`Unigram model`
    
        ```
        For each of the N words w_n: 
                Choose a word w_n ～ p(w); //挑选单词符合分布p(w)
        
        其中：
            - N表示要生成的文档的单词的个数，
            - w_n表示生成的第n个单词w，
            - p(w)表示单词w的分布，可以通过语料进行统计学习得到
        
        这种方法通过训练语料获得一个单词的概率分布函数，然后根据这个概率分布函数每次生成一个单词，使用这个方法M次生成M个文档
        ```
        
        ![uni_model](img/uni_model.png)
        
        - 每一篇文档的概率是组成这篇文档所有词的概率之乘积
        - M篇文档，每一篇文档有N个词，词与词之间是相互独立存在的。
    - 基础模型2：`Mixture of unigram`
        - unigram模型的方法的缺点就是生成的文本没有主题，文档直接被分解成词，过于简单，mixture of unigram方法对其进行了改进，给文档一个主题，并且从这个主题中去按照一定概率分布去选择所需要的词，该模型使用下面方法生成1个文档
        
            ```
            Choose a topic z ～ p(z); //从主题分布中选择一个主题
            For each of the N words w_n: 
                    Choose a word w_n ～ p(w|z); 
            
            其中：
                - z表示一个主题，p(z)表示主题的概率分布，z通过p(z)按概率产生；
                - N和w_n同上；
                - p(w|z)表示给定z时w的分布
            
            可以看成一个k×V的矩阵，k为主题的个数，V为单词的个数，每行表示这个主题对应的单词的概率分布，即主题z所包含的各个单词的概率，
            通过这个概率分布按一定概率生成每个单词。
            ```
        - 这种方法首先选选定一个主题z，主题z对应一个单词的概率分布`p(w|z)`，每次按这个分布生成一个单词，使用M次这个方法生成M份不同的文档
        - 该方式只允许一个文档只有一个主题，这不太符合常规情况，通常一个文档可能包含多个主题
    - 基础模型3：`LDA(Latent Dirichlet Allocation)`
        - LDA方法使生成的文档可以包含多个主题，该模型使用下面方法生成1个文档：
        
            ```
            Choose parameter θ ～ p(θ);
                        For each of the N words w_n:
                                Choose a topic z_n ～ p(z|θ);
                                Choose a word w_n ～ p(w|z);
            
            其中θ是一个主题向量，向量的每一列表示每个主题在文档出现的概率，该向量为非负归一化向量
                - p(θ)是θ的分布，具体为Dirichlet分布，即分布的分布；
                - N和w_n同上；
                - z_n表示选择的主题，
                - p(z|θ)表示给定θ时主题z的概率分布，具体为θ的值，即p(z=i|θ)= θ_i；
                - p(w|z)同上
            
            过程如下：
                - 这种方法首先选定一个主题向量θ，确定每个主题被选择的概率。
                - 然后在生成每个单词的时候，从主题分布向量θ中选择一个主题z，按主题z的单词概率分布生成一个单词。
            ```
 - ----------------------------- 分割线 -----------------------------
 - 在LDA主题模型中，一篇文档生成的方式如下：
 
    ![lda_doc](img/lda_doc.png)
 - 结构图：
 
    ![lda_arch](img/lda_arch.png)
 - LDA比pLSA对比：
    - 通常而言，LDA比pLSA效果更好，因为它可以轻而易举地泛化到新文档中去。
    - 在pLSA中，文档概率是数据集中的一个固定点。如果没有看到那个文件，我们就没有那个数据点。
    - 然而，在LDA中，数据集作为训练数据用于文档-主题分布的狄利克雷分布。即使没有看到某个文件，我们可以很容易地从狄利克雷分布中抽样得来，并继续接下来的操作。
    - 自己的话说就是：
        - PLSA适用于现成的语料，LDA适合有没见过的文档时效果较好，其他情况下LDA和PLSA效果类似，
        - 两者不同点在于LDA需要对主题和词分别做两次分布，PLSA比较直接，分别做一次分布就行了，不需要先验知识进行修正
 - 通过使用LDA，我们可以从文档语料库中提取人类可解释的主题，其中每个主题都以与之关联度最高的词语作为特征。
    - 例如，主题2可以用诸如「石油、天然气、钻井、管道、楔石、能量」等术语来表示。
    - 此外，在给定一个新文档的条件下，我们可以获得表示其主题混合的向量，例如，5％的主题1，70％的主题2，10％的主题3等。
    - 通常来说，这些向量对下游应用非常有用。
 - ----------------------------- 分割线 -----------------------------
 - LDA相对于PLSA的改进：
     - 在LDA中，每一篇文档都被看做是有一系列主题，在这一点上和pLSA是一致的。
     - 实际上，LDA的不同之处在于，**pLSA的主题的概率分布`P(c|d)`是一个确定的概率分布**，也就是虽然主题c不确定，但是c符合的概率分布是确定的，比如符合高斯分布，这个高斯分布的各参数是确定的
     - **但是在LDA中，这个高斯分布都是不确定的，高斯分布又服从一个狄利克雷先验分布**，说的绕口一点是主题的概率分布的概率分布（意思就是，主题的概率分布不固定，PLSA中每次选主题都是按照一个固定的概率分布去选择，而在LDA中每次选主题需要先按照一定概率选择一个主题分布，然后再按照这个主题分布去选择主题）
     - 除了主题有这个特点之外，另外词在主题下的分布也不再是确定分布，同样也服从一个狄利克雷先验分布。
     - 所以实际上LDA是pLSA的改进版，延伸版。
     - LDA对新文档有较好的泛化能力，PLSA只对已有的语料表现好
 - 这个改进有什么好处呢？原因就是就是我们上文说的pLSA容易过拟合，为什么容易过拟合？
    - pLSA中，主题的概率分布和词在主题下的概率分布既然是概率分布，那么就必须要有样本进行统计才能得到这些概率分布。
    - 更具体的讲，主题模型就是为了做这个事情的，训练已获得的数据样本，得到这些参数，那么一个pLSA模型便得到了
    - 但问题是：这些参数是建立在训练样本上得到的。不一定能确保新加入的数据同样符合这些参数。
    - 频率学派认为参数是存在并且是确定的，只是我们未知而已，并且正是因为未知，我们才去训练pLSA的，训练之后得到的参数同样适合于新加入的数据，因为他们相信参数是确定的，既然适合于训练数据，那么也同样适合于新加入的数据了。
    - 但是真实情况却不是这样，**尤其是训练样本量比较少的情况下的时候**，这个时候首先就不符合大数定律的条件：
        
        ```
        大数定律：在无数次独立同分布的随机事件中，事件的频率趋于一个稳定的概率值，这是大数定律
        中心极限定律：而同样的无数次独立同分布的随机事件中，事件的分布趋近于一个稳定的正态分布，而这个正态分布的期望值正是大数定律里面的概率值。
        
        所以，中心极限定理比大数定律揭示的现象更深刻，同时成立的条件当然也要相对来说苛刻一些。
        所以频率并不能很好的近似于概率，所以得到的参数肯定不好。
        ```
    - 我们都知道，概率的获取必须以拥有大量可重复性实验为前提，但是这里的主题模型训练显然并不能在每个场景下都有大量的训练数据。所以，**当训练数据量偏小的时候，pLSA就无可避免的陷入了过拟合的泥潭里了**。
    - 为了解决这个问题，LDA给这些参数都加入了一个先验知识，**就是当数据量小的时候，我人为的给你一些专家性的指导，你这个参数应该这样不应该那样**。
    
        ```
        比如你要统计一个地区的人口年龄分布，假如你手上有的训练数据是一所大学的人口数据，统计出来的结果肯定是年轻人占比绝大多数，
        这个时候你训练出来的模型肯定是有问题的，但是我现在加入一些先验知识进去，专家认为这个地区中老年人口怎么占比这么少？
        不行，我得给你修正修正，这个时候得到的结果就会好很多。所以LDA相比pLSA就优在这里，它对这些参数加入了一些先验的分布进去。
        ```
    - 但是，**当训练样本量足够大，pLSA的效果是可以等同于LDA的，因为过拟合的原因就是训练数据量太少**，当把数据量提上去之后，过拟合现象会有明显的改观。
 - LDA的公式如下：
 
    ![lda_form](img/lda_form.png)
 - LDA形象化的描述：
 
    ![lda_img](img/lda_img.png)
    
    ```
    首先，我们假设一篇文档含有3个词，待选主题也是3个，为什么选择3？主要是是为了好用三角形来表示他们的关系，
    当然如果你有很好的超空间想象能力，你也可以选择更多的词来描述。
    
    外边最大的三角形是word simplex，其内的每一个点都表示一个概率分布，也就是产生这三个词的概率大小。比如p2这个点，
    它产生word 1的概率是，p2点到word 1对边的距离，也就是p2到底边的距离。由于等边三角形内的任何一点到三条边的距离
    之和等于高，相当于概率之和为1类似，这就非常好的保证了用距离来度量概率大小的准确性。
    ```
    
    ![lda_form_1](img/lda_form_1.png)
    
 每次产生文档的时候都是按照dirichlet分布来确定一个主题分布的分布和词分布的分布，然后剩下的就和pLSA相同了


**代码实现：**
```
from gensim.corpora.Dictionary import load_from_text, doc2bow
from gensim.corpora import MmCorpus
from gensim.models.ldamodel import LdaModel
document = "This is some document..."
# load id->word mapping (the dictionary)
id2word = load_from_text('wiki_en_wordids.txt')
# load corpus iterator
mm = MmCorpus('wiki_en_tfidf.mm')
# extract 100 LDA topics, updating once every 10,000
lda = LdaModel(corpus=mm, id2word=id2word, num_topics=100, update_every=1, chunksize=10000, passes=1)
# use LDA model: transform new doc to bag-of-words, then apply lda
doc_bow = doc2bow(document.split())
doc_lda = lda[doc_bow]
# doc_lda is vector of length num_topics representing weighted presence of each topic in the doc
```

**主题模型非负矩阵分解（NMF）**
 - TODO


#### 深度学习中的LDA：lda2vec
**一般概念：**
 - 在文章的开头，我们谈到能够从每个级别的文本（单词、段落、文档）中提取其含义是多么重要。
 - 在文档层面，我们现在知道如何将文本表示为主题的混合。
 - 在单词级别上，我们通常使用诸如word2vec之类的东西来获取其向量表征。
 - lda2vec是word2vec和LDA的扩展，它共同学习单词、文档和主题向量
 - lda2vec专门在word2vec的skip-gram模型基础上建模，以生成单词向量。
 - skip-gram和word2vec本质上就是一个神经网络，通过利用输入单词预测周围上下文词语的方法来学习词嵌入

**lda2vec：**
 - 通过使用lda2vec，我们不直接用单词向量来预测上下文单词，而是使用上下文向量来进行预测。
 - 该上下文向量被创建为两个其它向量的总和：单词向量和文档向量。
    - 单词向量由前面讨论过的skip-gram word2vec模型生成
    - 而文档向量更有趣，它实际上是下列两个组件的加权组合：
        - 文档权重向量，表示文档中每个主题的「权重」（稍后将转换为百分比）
        - 主题矩阵，表示每个主题及其相应向量嵌入
    - 文档向量和单词向量协同起来，为文档中的每个单词生成「上下文」向量。
 - lda2vec的强大之处在于，它不仅能学习单词的词嵌入（和上下文向量嵌入），还同时学习主题表征和文档表征。
    
    ![lda2vec](img/lda2vec.png)


































## ES

参考博客：
 - [终于有人把Elasticsearch原理讲透了！](https://www.zhihu.com/search?type=content&q=elasticsearch%E5%92%8Clda)

**一般概念：**
 - 反向索引又叫倒排索引，是根据文章内容中的关键字建立索引。
 - 搜索引擎原理就是建立反向索引。
 - Elasticsearch在Lucene的基础上进行封装，实现了分布式搜索引擎。
 - Elasticsearch中的索引、类型和文档的概念比较重要，类似于MySQL中的数据库、表和行。



## 参考文献
 - [An Introduction to Neural Information Retrieval](https://www.microsoft.com/en-us/research/uploads/prod/2017/06/fntir2018-neuralir-mitra.pdf)
 - [Introduction to Information Retrieval](https://nlp.stanford.edu/IR-book/)
 - [一文读懂如何用LSA、PSLA、LDA和lda2vec进行主题建模](https://www.zhihu.com/search?type=content&q=LSA)
 - [LDA主题模型笔记](https://www.cnblogs.com/yang901112/p/11644930.html)
 - [主题模型-LDA浅析](https://blog.csdn.net/huagong_adu/article/details/7937616)
 - [LDA主题模型详解](https://blog.csdn.net/daycym/article/details/88876460)
 - [既然 LDA 是一种比 PLSA 更高级的模型，为啥百度还在用 PLSA？](https://www.zhihu.com/search?type=content&q=%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B)
