# 语言模型

> 目前主要采用的是n元语法模型，这种模型构建简单、直接，但同时也因为数据缺乏 而必须采取平滑（smoothing）算法

**参考文献：**
 - 《统计自然语言处理》 by 宗成庆
 - 《深度学习》(花书) by Bengio
 - 《神经网络与深度学习》 by 邱锡鹏 第14章


**一般概念：**
 - 词的局部表示就是one-hot向量表示，即词袋模型(Bag-of-Words, BOW)，即将文本看成是词的集合，不考虑词序信息。又叫做向量空间模型(Vector Space Model, VSM)
 - 将高维的局部表示向量空间`R^{|V|}`映射到一个非常低维的空间`R^d`，V为词表，。这个低维空间中的表示就是**分布式表示**
 - 对于词的分布式表示（即低维稠密向量表示），我们经常叫做词嵌入（Word Embeddings）
 - 在机器学习中，**嵌入**通常指将一个度量空间X中的对象映射到另一个低维的度量空间Y中，并进行可能保持不同对象之间拓扑关系。
 - 在one-hot向量空间中，每个词都位于坐标轴上，每个坐标轴上一个词；而在低维的嵌入空间中，每个词都不在坐标轴上，词之间可以计算相似度。

## 统计语言模型
**定义：**
 - 统计语言模型把语言（词/字/字符的序列）看作一个随机事件，并赋予相应的概率来描述其属于某种语言集合的可能性。
 - 给定一个词汇集合V，对于一个由V中的词构成的序列`S=⟨w1,···,wT⟩ ∈ V^n`，统计语言模型赋予这个序列一个概率`P(S)`，来衡量S符合自然语言的语法和语义规则的置信度。

简单地说，统计语言模型就是用来**计算一个句子的概率的模型**。统计语言模型可以估计出自然语言中每个句子出现的可能性，而不是简单的判断该句子是否符合文法。

**一些性质：**
 - 语言模型的两个基本功能是：
    - 判断一段文本是否符合一种语言的语法和语义规则
    - 生成符合一种语言语法或语义规则的文本
 - [多项分布](https://baike.baidu.com/item/%E5%A4%9A%E9%A1%B9%E5%88%86%E5%B8%83/6548502?fr=aladdin)。
 多项式分布是二项式分布的推广。二项分布的典型例子是扔硬币，硬币正面朝上概率为p,重复扔n次硬币，k次为正面的概率即为一个二项分布概率。把二项分布公式推广至多种状态，就得到了多项分布。
 - 在传统的统计语言模型中，一般**假设语言是服从多项分布**，并**利用最大似然估计来求解多项分布的参数**(也就是求解前n-1个词的条件下，第n个词的概率)。
 - **最大似然估计等价于频率估计**


**N元语法：**
 - 一个语言模型**通常构建为字符串s的概率分布`p(s)`，试图反应的是字符串s作为一个句子出现的频率。**
 - 与语言学中不同，语言模型与句子是否合乎语法是没有关系的，即使一个句子完全合乎语法逻辑，我们仍然可以认为它出现的概 率接近为零
 - 句子`s=w_{1}w_{2}...w_{l}`，其概率计算公式可以表示为：
     ```
     p(s) = p(w_1)p(w_2|w_1)p(w_3|w_1w_2)...p(w_l|w_1...w_{l-1})
          = \Pi_{i=1}^l p(w_i|w_1...w_{i-1})
     ```
 - n元语法（或n元文法，n-gram），用来解决当前词的历史词过多时计算上述公式参数过多的问题
    - 产生第i个词的概率是由已经产生的i-1个词决定的，把前边的i-1个词成为第i个词的历史
    - 随着历史长度的增加，不同的历史数目按指数级增长，这使我们几乎不可能从训练数据中正确地估计出这些参数，实际上，绝大多数历史根本就不可能在训练数据中出现。
    - 为了解决这个问题，可以将历史w1w2…wi-1按照某个法则映射到等价类E(w1w2…wi-1)，而等价类的数目远远小于不同历史的数目。
    - 如果假定`p(w_i|w_1,w_2,...,w_{i-1}) = p(w_i|E)`，那么，自由参数的数目就会大大地减少
    - 一种比较实际的做法是，将两个历史映射到同一个等价类，当且仅当这两个历史**最近的`n-1`个词相同**
    - 满足上述条件的语言模型称为n元语法
    - 通常情况下，n的取值不能太大，否则，等价类太多，自由参数过多的问题仍然存在。
        - 在实际应用中，取n=3的情况较多（即当前词仅与它之前的前两个词相关，3-gram）。
        - 当n=1时，即出现在第i位上的词w_i独立于历史时（不与之前的任何词相关），一元文法被记作unigram，或uni-gram，或monogram
        - 当n=2时，即出现在第i位上的词w_i仅与它前面的一个历史词w_{i-1}有关，**二元文法模型被称为一阶马尔可夫链**(Markov chain)，记作bigram或bi-gram
        - 当n=3时，即出现在第i位上的词w_i仅与它前面的两个历史词w_{i-2}w_{i-1}有关，**三元文法模型被称为二阶马尔可夫链**，记作trigram或tri-gram
    - 以二元语法模型为例，根据前面的解释，我们可以近似地认为，一个词的概率只依赖于它前面的一个词，那么概率计算公式近似为：
        ```
        p(s) = \Pi_{i=1}^l p(w_i|w_1...w_{i-1})
             = \Pi_{i=1}^l p(w_i|w_{i-1})
        ```
        - 为了让上式对于i=1有意义，我们在句子开头加上一个句首标记<BOS>，即假设w_0就是<BOS>
        - 为了使得所有的字符串的概率之和`sum_{s}p(s)`等于1，需要在句子结尾再放一个句尾标记<EOS>，并且使之包含在上边等式的乘积中，
        (如果不做这样的处理，所有给定长度的字符串的概率和为1，而所有字符串的概率和为无穷大，因为不知道什么时候是句子的结束，当知道句子的长度时，在知道在哪结束句子)
        - 为了估计条件概率`p(w_i|w_{i-1})`，可以简单地计算二元语法`w_{i-1}w_i`在某一文本中**出现的频率**，然后归一化。
        如果用`c(w_{i-1}w_i)`表示二元语法`w_{i-1}w_i`**在给定文本中的出现次数**，我们可以采用下面的计算公式:
            ```
            p(w_i|w_{i-1}) = c(w_{i-1}w_i) / sum_{w_i}c(w_{i-1}w_i)
            ```
        分母`sum_{w_i}c(w_{i-1}w_i)`表示计算历史`c(w_{i-n+1}^{i-1})`的数目，具体见下边例子
        - 用于构建语言模型的文本称为训练语料。对于n元语法模型，使用的训练语料的规模一般要有几百万个词。上式用于估计`p(w_i|w_{i-1})`的方法称为`p(w_i|w_{i-1})`的**最大似然估计**
        - 计算例子：
            ```
            假设语料S由下边的3个句子构成：
            ("BROWN READ HOLY BIBLE", 
             "MARK READ A TEXT BOOK", 
             "HE READ A BOOK BY DAVID")
             
            用计算最大似然估计的方法计算概率 p(BROWN READ A BOOK)：
            p(BROWN|<BOS>) = c(<BOS> BROWN) / sum_{w}c(<BOS> w) = 1/3
            p(READ|BROWN) = c(BROWN READ) / sum_{w}c(BROWN w) = 1/1
            p(A|READ) = c(READ A) / sum_{w}c(READ w) = 2/3
            p(BOOK|A) = c(A BOOK) / sum_{w}c(A w) = 1/2
            p(<EOS>|BOOK) = c(BOOK <EOS>) / sum_{w}c(BOOK w) = 1/2
            
            因此：
            p(BROWN READ A BOOK) = p(BROWN|<BOS>) ×
                                   p(READ|BROWN) ×
                                   p(A|READ) ×
                                   p(BOOK|A) ×
                                   p(<EOS>|BOOK)
                                 = 1/3 × 1/1 × 2/3 × 1/2 × 1/2
                                 = 0.06
            ```

**n-gram与马尔可夫链的关系：**
 - 二元文法模型被称为一阶马尔可夫链，三元文法模型被称为二阶马尔可夫链

**困惑度：**
 - 困惑度(Perplexity)可以用来衡量一个分布的不确定性，我们也可以用困惑度来衡量两个分布之间差异
 - 对于一个未知的数据分布`P_{data}(X)`，和一个模型分布`P_{model}(X)`，我们从`P_{data}(X)`中采样出一组测试样本`x1,···,xN`，模型分布`P_{model}(X)`的困惑度为：
     ```
     2^{ H(P'_{data}, P_{model}) } = 2^{-1/N sum_{i=1}^N log_{2}P_{model}(x_i)}
     
     其中H(P'_{data}, P_{model})为样本的经验分布P'_{data}和模型分布P_{model}之间的交叉熵，  
     也是所有样本上的负对数似然函数
     ```
 - 模型的困惑度**可以衡量模型估计分布与样本经验分布之间的契合程度**，困惑度越低则两个分布越接近。因此，模型`P_{model}`的好坏可以用困惑度来评价。

**语言模型性能评价：**
 - 一个好的语言模型应该使得测试集合中的**句子的联合概率尽可能高**
 - 语言模型最直接的评价就是测试集中所有句子的联合概率，一个好的语言模型在测试集上的概率越高越好。但是联合概率的缺点是一般都比较小，并且和句子长度有关。越长的句子，其联合概率越小。因此，在语言模型中，经常使用困惑度来评价。
 - 根据模型计算出的测试数据的概率来评价一个语言模型，或用交叉熵和困惑度等派生测度。
 - 给定一个语言模型，文本T的概率为`p(T)`，在数据T上模型p的交叉熵`H_{p}(T)`定义为：
     ```
     H_{p}(T) = - (1/W_{T}) log_{2}p(T)
     ```
 这里的W_T是以词为单位度量的文本T的长度（可以包括句首标志<BOS>或句尾标志<EOS>）
 - 模型p的困惑度`PP_T(T)`是模型分配给测试集T中每一个词汇的概率的几何平均值的倒数，它和交叉熵的关系为：
     ```
     PP_T(T) = 2^{Hp(T)}
     ```
 显然，交叉熵和困惑度越小越好，这是我们评估一个语言模型的基本准则。
 - 困惑度为每个词条件概率`P(w_{j}^{(i)} | w_{(j−n+1):(j−1)}^{(i)})`的几何平均数的倒数。句子概率越大，困惑度越小，语言模型越好。
 - 假设一个语言模型赋予每个词出现的概率均等，则该语言模型的困惑度为`|V|`

**数据平滑：**
 - 基于统计的语言模型最大似然估计存在一个问题，**数据稀疏问题**，主要是由于训练样本不足而导致密度估计不准确
 - [Zipf定律](https://baike.baidu.com/item/%E9%BD%90%E5%A4%AB%E5%AE%9A%E5%BE%8B/6643264?fr=aladdin)，
 在自然语言的语料库里，一个单词出现的频率与它在频率表里的排名成反比。所以，频率最高的单词出现的频率大约是出现频率第二位的单词的2倍，而出现频率第二位的单词则是出现频率第四位的单词的2倍。
 - 数据稀疏问题最直接的解决方法就是增加训练语料的规模，但是由于大多数自然语言都服从Zipf定律，增加语料库规模的边际效益会随着语料库的增加而递减。
 - 数据稀疏问题的另一种解决方法是平滑技术
 - 在计算条件概率过程中，根据当前的语料S，如果一个词在语料库里不存在，可能会出现概率为0的情况，显然，这个结果不够准确，因为句子`DAVID READ A BOOK`总有出现的可能，其概率应该大于0
 - 因而，必须分配给所有可能出现的字符串一个非零的概率值来避免这种错误的发生
 - 平滑（smoothing）技术就是用来解决这类零概率问题的。术语“平滑”指的是为了产生更准确的概率**来调整最大似然估计**的一种技术，也常称为数据平滑。
 - **“平滑”处理的基本思想是“劫富济贫”，即提高低概率（如零概率），降低高概率，尽量使概率分布趋于均匀**
 - 对于二元语法来说，一种最简单的平滑技术就是假设每个二元语法出现的次数比实际出现的次数多一次，不妨将该处理方法称为**加1法平滑法**，分子加1,分母加`|V|`（当前语料S中，词汇表单词的个数）
 - 数据平滑方法：
    - 加法平滑法
        - 是最简单的，使加1平滑方法通用化
        - 不是假设每一个n元语法发生的次数比实际统计次数多一次，而是假设它比实际出现情况多发生δ次，`0≤δ≤1`
    - 古德-图灵估计法(Good-Turing)
        - Good-Turing估计法是很多平滑技术的核心
        - 其基本思路是：对于任何 一个出现r次的n元语法，都假设它出现了`r*`次
        - Good-Turing方法不能直接用于估计`n_r＝0`的n-gram概率，其中，`n_r`是训练语料中恰好出现r次的n元语法的数目
        - Good-Turing方法不能实现高阶模型与低阶模型的结合，而高低阶模型的结合通常是获得较好的平滑效果所必须的
    - Katz平滑法
        - Katz平滑方法通过加入高阶模型与低阶模型的结合，扩展了Good-Turing估计方法。
        - 所有具有非零计数r的二元语法都根据折扣率`d_r`被减值了
        - Katz平滑方法属于后备（back-off）平滑方法。这种方法的中心思想是，当某一事件在样本中出现的频率大于k时，运用最大似然估计经过 减值来估计其概率。当某一事件的频率小于k时，使用低阶
        的语法模型作为代替高阶语法模型的后备，而这种代替必须受归一化因子α的作 用。对于这种方法的另一种解释是，根据低阶的语法模型分配由于减值而节省下来的剩余概率给未见事件，这比将剩余概率平均分配给未见事件要合理
    - Jelinek-Mercer平滑方法
        - 因为冠词THE要比单词THOU出现的频率高得多。为了利用这种情况， 一种处理办法是在二元语法模型中加入一个一元模型。
        - 一般来讲，使用低阶的n元模型向高阶n元模型插值是有效的，因为当没有足够的语料估计高阶模型的概率时，低阶模型往往可以提供有用 的信息。
        - 第n阶平滑模型可以递归地定义为n阶最大似然估 计模型和n-1阶平滑模型之间的线性插值
    - Witten-Bell平滑方法
        - 它可以认为是Jelinek-Mercer平滑算法的一个实例
        - n阶平滑模型被递归地定义为n阶最大似然模型和n-1阶平滑模型的线性插值
    - 绝对减值法
        - 绝对减值法（absolute discounting）类似于Jelinek-Mercer平滑算法，涉及高阶和低阶模型的插值问题
        - 通过从每个非零计数中减去一个固定值`D≤1`的方法来建立高阶分布
    - Kneser-Ney平滑方法
    - Church-Gale平滑方法
    - 贝叶斯平滑方法
    - 修正的Kneser-Ney平滑方法
 - 平滑方法的比较：
    - **不管训练语料规模多大，对于二元语法和三元语法而言，Kneser-Ney平滑方法和修正的Kneser-Ney平滑方法的效果都好于其他所有的平滑方法**。
    - 一般情况下，Katz平滑方法和Jelinek-Mercer平滑方法也有较好的表现，但与上述两者平滑方法相比稍有逊色。
    - 在稀疏数据的情况下，Jelinek-Mercer平滑方法优于Katz平滑方法，而在有大量数据的情况下，Katz平滑方法则优于Jelinek-Mercer平滑方法
    - 插值的绝对减值平滑方法和后备的Witten-Bell平滑方法的表现最差
    - 除了对于很小的数据集以外，插值的绝对减值平滑方法一般优于留存插值方法的Jelinek-Mercer平滑方法
    - 而Witten-Bell平滑方法则表现较差，对于较小的数据集，该方法比留存插值方法的Jelinek-Mercer平滑方法差得多
    - 对于大规模数据集而言，这Witten-Bell平滑方法和插值的绝对减值平滑方法都比留存插值方法的Jelinek-Mercer平滑方法优越得多，甚至可以与Katz平滑方法和Jelinek-Mercer平滑方法相匹敌
    - 平滑方法的相对性能与训练语料的规模、n元语法模型的阶数和训练语料本身有较大的关系，其效果可能会随着这些因素的不同而出现很大的变化
    - 下列因素对于平滑算法的性能有一定的影响：
        - 影响最大的因素是采用修正的后备分布
        - 绝对减值优于线性减值
        - 从性能上来看，对于较低的非零计数，插值模型大大地优于后备模型
        - 增加算法的自由参数，并在留存数据上优化这些参数，可以改进算法的性能

**语言模型的自适应方法：**
 - 经常遇到的问题：
    - **模型对跨领域的脆弱性：**在训练语言模型时所采用的语料往往来自多种不同的领域，这些综合性语料难以反映不同领域之间在语言使用规律上的差异，而**语言模型恰恰对于训练文本的类型、主题和风格等都十分敏感**
    - **独立性假设的无效性：**n元语言模型的独立性假设前提是一个文本中的当前词出现的概率只与它前面相邻的n-1个词相关，但**这种假设在很多情况下是明显不成立的**。
 - 香农实验表明，相对而言，人更容易运用特定领域的语言知识、常识和领域知识进行推理以提高语言模型的性能（预测文本的下一个成分）。因此，**为了提高语言模型对语料的领域、主题、类型等因素的适应性，提出了自适应语言模型**
 - 语言模型自适应方法：
    - 基于缓存的语言模型
        - 在文本中刚刚出现过的一些词在后边的句子中再次出现的可能性往往较大，比标准的n元语法模型预测的概率要大
        - 针对这种现象，cache-based自适应方法的基本思路是，语言模型通过n元语法的线性插值求得，插值系数λ可以通过EM算法求得
        - （正常的基于缓存的语言模型）常用的方法是，在缓存中保留前面的K个单词，每个词wi的概率（缓存概率）用其在缓存中出现的相对频率计算得出
        - （衰减的基于缓存的语言模型）缓存中每个词对当前词的影响随着与该词距离的增大呈指数级衰减，越是临近的词，对缓存概率的贡献越大
        - cache-based自适应方法减低了语言模型的困惑度，衰减的比正常的给予缓存的语言模型对降低语言模型的困惑度效果更好
        - 黄非等（1999）提出了利用特定领域中少量自适应语料，在原词表中通过**分离通用领域词汇和特定领域词汇，并自动检测词典外领域关键词实现词典自适应**，然后结合基于缓存的方法实现语言模型的自适应方法
        - 曲卫民等（2003）通过采用TF-IDF公式代替原有的简单频率统计法，**建立基于记忆的扩展二元模型，并采用权重过滤法以节省模型计算量**，实现了对基于缓存记忆的语言模型自适应方法的改进
        - 张俊林等 （2005）也对基于记忆的语言模型进行了扩展，利用汉语义类词典，**将与缓存中所保留词汇语义上相近或者相关的词汇也引入缓存**，在一定程度上提高了原有模型的性能
    - 基于混合方法的语言模型
        - 基于混合方法的自适应语言模型针对的问题是：
            - 由于大规模训练语料本身是异源的，来自不同领域的语料无论在主题方面，还是在风格方面，或者同时在这两方面都有一定的差异
            - 而测试语料一般是同源的，因此，为了获得最佳性能，语言模型必须适应各种不同类型的语料对其性能的影响。
        - 基本思想是：
            - 将语言模型划分成n个子模型M1，M2，…，Mn，整个语言模型的概率通过下面的线性插值公式计算得到
                ```
                p(w_i|w_{1}^{i-1}) = sum_{j=1}^n lambda_j p_{M_j}(w_i|w_{1}^{i-1})
                ```
            lambda属于0到1，之和为1，可以通过EM算法计算
        - 基于这种思想，该适应方法针对测试语料的实现过程包括下列步骤：（即有针对性的训练子语言模型）
            - 对训练语料按来源、主题或类型等（不妨按主题）聚类；
            - 在模型运行时识别测试语料的主题或主题的集合；
            - 确定适当的训练语料子集，并利用这些语料建立特定的语言模型；
            - 利用针对各个语料子集的特定语言模型和线性插值公式，获得整个语言模型。
    - 基于最大熵的语言模型
        - 上面两种思路都是分别建立各个子模型，然后将子模型的输出组合起来
        - 基于最大熵的语言模型采用不同的思路，**即通过结合不同信息源的信息构建一个语言模型。每个信息源提供一组关于模型参数的约束条件，在所有满足约束的模型中，选择熵最大的模型**
        - 例子：
            ```
            考虑两个语言模型M1和M2，假设M1是标准的二元模型，表示为f函数：
            p_{M_1}(w_i|w_{1}^{i-1}) = f(w_i, w_{i-1})
            
            M2是距离为2的二元模型（distance-2 bigram），定义为g函数
            p_{M_2}(w_i|w_{1}^{i-1}) = g(w_i, w_{i-2})
            
            可以用线性插值方法取这两个概率的平均值，用后备方法选择其中一个进行数据平滑。
            
            为了考虑**远距离的文本历史信息**，以弥补一般语言模型仅仅利用近距离历史的不足，采用**触发器对**作为信息承载成分的思想。
             - 如果一个词序列A与另一个词序列B密切相关，那么，A→B被看作一个触发器对，
             - 其中，A为触发器，B为被触发的序列。
             - 当A出现在一个文本中时，它触发B，从而引起B的概率估计发生变化。
             - 如果把B看作当前词，A为历史h中的某个特征，那么，可以将一个二值的触发器对A→B形式化为一种约束函数f_{A→B}
            ```
        - 由于最大熵模型能够较好地将来自不同信息源的模型结合起来，获得性能较好的语言模型,因此，有些学者研究将基于主题的语言模型 （topic-based LM）（主题条件约束）与n元语法模型
        相结合，用于对话语音识别、信息检索IR和隐含语义分析(LSA)
 - 语言模型的自适应方法是改进和提高语言模型性能的重要手段之一。其性能表现与语料本身的状况（领域、主题、风格等）以及选用的统计基元等密切相关


## 神经网络语言模型
**一般概念：**
 - 在统计语言模型中，一个关键的问题是估计`P(Wt|W_{1:(t−1)})`，即在时刻（或位置）t，给定历史信息`h_t = w_{1:(t−1)}`条件下，词汇表V中的每个词v_k出现的概率。
 这个问题可以转换为一个类别数为|V|的多类分类问题，估计的词汇表中第k个词出现的后验概率。
 - 这样，我们就可以**使用机器学习中的不同分类器来估计语言模型的条件概率**。这里我们关注基于神经网络模型的方法，这一类方法可以统称为神经网络语言模型（NNLM）
 
**神经网络语言模型:**
 - 神经网络语言模型可以分为三个部分：
    - 输入层
        - 将语言符号序列`w_{1:(t-1)}`输入到神经网络模型中（以词嵌入矩阵的形式，稠密实数值向量）
        - 词嵌入矩阵`M ∈ R^{d1×|V|}`中，第k列向量`k ∈ R^{d1}`表示词汇表中第k个词对应的稠密向量
        - 通过直接映射，得到历史信息`w_{1:(t-1)}`每个词对应的向量表示`v_{w1},···,v_{w_{t−1}}`
    - 隐藏层
        - 隐藏层可以是不同类型的网络，前馈神经网络和循环神经网络，其输入为词向量`v_{w1},···,v_{w_{t−1}}`，输出为一个可以表示历史信息的向量`h_t`
        - 常见的网络类型：
            - 简单平均
                ```
                h_t = sum_{1}^{t-1} C_i v_{w_i}
                
                其中，C_i为每个词的权重。权重可以和位置相关，也可以和位置无关。位置无关的权重可以设置为C_i = 1/(t−1)
                ```
            - 前馈神经网络
                - 前馈神经网络要求输入的大小是固定的。
                - 因此，和n元模型类似，假设历史信息只包含前面n−1个词。首先将这n−1个的词向量拼接为一个维度为`d1 × (n−1)`的向量`x_t`
                - 然后将`x_t`输入到由多层前馈神经网络构成的隐藏层，最后一层隐藏层的输出`h_t`
                - 前馈网络的结构可以任意设置：
                    - 比如，只含一层隐藏层的模型为`h_t = tanh(W x_t + c)`
                    - 也可以包含跳层连接`h_t = x_t ⊕ tanh(W x_t + c)`
            - 循环神经网络
                - 和前馈神经网络不同，循环神经网络可以接受变长的输入序列，依次接受输入`v_{w1},···,v_{w_{t−1}}`，得到t时刻的隐藏状态`h_t`
        - 前馈网络语言模型和循环网络语言模型的不同之处在于**循环神经网络利用隐藏状态来记录以前所有时刻的信息，而前馈神经网络只能接受前(n-1)个时刻的信息。
    - 输出层
        - 输出层为大小为`|V|`，其接受的输入为历史信息的向量表示`h_t ∈ R^{d2}`，输出为**词汇表中每个词的后验概率**。
        - 在神经网络语言模型中，一般使用`softmax`分类器
            ```
            y_t = softmax(O h_t + b)
            
            输出向量y_t ∈ R^{|V|}为一个概率分布，
            O ∈ R^{|V|×d2}是最后一层隐藏层到输出层直接的权重。
            O也叫做**输出词嵌入矩阵**，矩阵中每一行也可以看作是一个词向量。
            ```
 - 大词汇表上`softmax`计算的改进 (加速语言模型训练速度)
    - 而归一化时要计算配分函数，即对词汇表中所有的词进行计算并求和，计算开销非常大，在实践中，经常采样一些近似估计的方法来加快训练速度。主要的加快训练速度的方法有两类：
    - 一类是**层次化的`softmax`计算**，将标准`softmax`函数的扁平结构转换为层次化结构
        - 先来考虑两层的来组织词汇表，即将词汇表中词分成k组，每一个词只能属于一个分组，每组大小为`|V|/k`。假设词w所属的组为`c(w)`，则
            ```
            P(w|h) = P(w,c(w)|h)
                   = P(w|c(w),h) × P(c(w)|h)
            
            其中，P(c(w)|h)是给定历史信息h条件下，类c(w)的概率，
            P(w|c(w), h)是给定历史信息h和类c(w)条件下，词w的概率
            一个词的概率可以分解为两个概率的乘积，而它们都是利用神经网络来估计，
            计算softmax函数时分别只需要做|V|/k和k次求和，从而就大大提高了softmax函数的计算速度
            ```
        - 一般对于词汇表大小|V|，我们将词平均分到`sqrt(|V|)`个分组中，每组`sqrt(|V|)`个词，这样通过一层的分组，我们可以将`softmax`计算加速`1/2 × sqrt(|V|)`倍。
        - 为了进一步降低`softmax`的计算复杂度，我们可以更深层的树结构来组织词汇表。假设用二叉树来组织词汇表中的所有词，二叉树的叶子节点代表词汇表中的词，非叶子节点表示不同层次上的类别。
        如果我们将二叉树上所有左链接标记为0，右链接标记为1。每一个词可以用根节点到它所在的叶子之间路径上的标记来进行编码。
        - 词v的条件概率为：
            ```
            P(v|h) = P(b(v,j)|n(v,j-1), h)
            
            n(v,j-1)是一个非叶子结点，
            因为b(v, j) ∈ {0, 1}，因此可以看作是二分类问题，可以使用logistic回归来进行计算
            若使用平衡二叉树来进行分组，则条件概率估计可以转换为log_{2}|V|个二分类问题，
            这时softmax函数可以用logistic函数代替,softmax的计算可以加速 |V|/(log_{2}|V|)倍
            ```
        - 将词汇表中的词按照树结构进行组织，有以下方式：
            - 利用人工整理的词汇层次结构，比如利用WordNetMiller系统中的`IS-A`关系（is a, 即上下位关系）。例如，“狗”是“动物”的下位词。因为WordNet的层次化结构不是二叉树，因此需要通过进一步聚类来转换为二叉树。
            - 使用Huffman编码。**Huffman编码对出现概率高的词使用较短的编码，出现概率低的词则使用较长的编码。因此训练速度会更快。**
    - 另一类是**基于采样的方法**，通过采样来近似计算更新梯度
        - 神经网络语言模型的目标函数，如果用随机梯度下降来求解参数θ，在计算每个样本的更新梯度时需要**两次计算在整个词汇表上的求和**，由于语言模型中的词汇表都比较大，训练速度非常慢
        - 为提高训练效率，可以用采样方法来近似地估计求导后公式中的期望，两种在语言模型中比较有效的采样方法：
            - 重要性采样
                - 用一个容易采样的参考分布Q，来近似估计分布P
                - 经过对公式的变换，将分布P上的期望转换为分布Q上的期望
                - 分布Q应与分布P尽可能接近，且从Q采样的代价要比较小
                - 在语言模型中，分布Q可以采用n元语言模型的分布函数
                - 有了分布Q之后，可以从中独立抽取k个样本来近似求解公式
                - 通过Q的采样来估计目标分布P的方法就叫做重要性采样
                - 重要性采样是一种非均匀采样
                - 最后，可以得到每个样本目标函数关于θ的梯度的近似值
                - 重要性采样相当于采样了一个词汇表的子集，然后在这个子集上求梯度的期望
                - 采样的数量n越大，近似越接近正确值，在实际应用中，n取100左右就能够以足够高的精度对期望做出估计
                - 通过重要性采样的方法，训练速度可以加速`|V|/k`倍，k为子词汇表的大小
                - 但是，其效果依赖于建议分布`Q(w′|h_t)`的选取，如果选取不合适，会造成梯度估计非常不稳定，经常使用一元模型的分布函数
            - 噪声对比估计
                - 基本思想是将密度估计问题转换为二分类问题，从而降低计算复杂度
                - 比如我们教小孩认识“苹果”，往往会让小孩从一堆各式各样的水果中找出哪个是“苹果”。通过不断的对比和纠错，最终小孩会知道了解“苹果”的特征，并很容易识别出“苹果”
                - 噪声对比估计的数学描述如下：
                    - 假设有三个分布，一个是需要建模真实数据分布`P(x)`
                    - 第二是模型分布`P_{θ}(x)`，我们期望调整模型参数为θ来使得`P_{θ}(x)`来拟合真实数据分布
                    - 第三个是噪声分布`Q(x)`，用来对比学习。
                    - 从`P(x)`和`Q(x)`的混合分布中抽取一个样本x，从`P(x)`中抽取的样本叫做“真实”样本，从`Q(x)`中抽取的样本叫做噪声样本。
                    - 需要建立一个“辨别者”D来判断样 本x是真实样本还是噪声样本。
                    - 噪声对比估计是通过调整模型`P_{θ}(x)`使得“判别者”D很容易能分别出样本x来自哪个分布。
                - 一般噪声样本的数量要比真实样本大很多。这里假设噪声样本的数量是真实样本的k倍
                - 噪声对比估计的目标是将模型分布`P_{θ}(x)`和噪声分布`Q(x)`区别开来，可以看作是二分类问题。通过不断比较真实样本和噪声样本，来学习模型参数θ。
                - 噪声对比估计相当于用判别式的准则`L_θ`来训练一个生成式模型`P_{θ}(x)`，其思想与生成式对抗网络类似。不同之处在于，在噪声对比估计中的“判别者”D是通过Bayes
                公式计算得到，而生成式对抗网络的“判别者”D是一个需要学习的神经网络。
                - 噪声对比估计是找到一个模型`P_{θ}(x)`使得“判别者”D很容易能分别出样本x来自哪个分布。
                - 噪声对比估计方法的一个特点是会促使未归一化分布`exp(s(w, h; θ))`可以自己学习到一个近似归一化的分布，并接近真实的数据分布`P(w|h)`
                - 在噪声对比估计中，噪声分布`Q(w)`的选取也十分关键
    - 总结：
        - 基于采样的方法并不改变模型的结构，只是近似计算更新梯度。因此这类方法虽然在训练时可以显著提高模型的训练速度，但是在测试阶段依然需要计算配分函数。
        - 而基于层次化`softmax`的方法改变的模型的结构，在训练和测试时都可以加快计算速度。

## 基于分布式假设的词嵌入学习
**一般概念：**
 - 通过神经网络语言模型，我们可以在大规模的无标注语料上进行训练，来得到一组好的词向量。
 - 这些词向量可以作为预训练的参数，再代入到特定任务中进行精调。
 - 使用NNLM来训练词嵌入有两个不足：
    - 一是即使使用改进的NNLM，其训练也需要大量的计算资源训练，训练时间非常长。
    - 二是NNLM的优化目标是降低语言模型的困惑度，和词嵌入的好坏并不是强相关关系。
 - 虽然训练一个好的语言模型会得到一组好的词嵌入，但是一组好的词嵌入却不一定要使得语言模型的困惑度降低。

下边几种是**不通过优化语言模型而直接学习词嵌入**的方法：

**连续词袋模型和Skip-Gram模型：**
 - 词嵌入学习工具word2vec中包含的两种模型：
    - 连续词袋模型和Skip-Gram模型
    - 这两种模型虽然依然是基于语言模型，但**训练目标是得到一组较好的词嵌入而不是降低语言模型的困惑度**。
    - 为了提高训练效率，这两种模型都通过简化模型结构大幅降低复杂度，并提出两种高效的训练方法（**负采样**和**层次化softmax**）来加速训练
    - 在标准的语言模型中，当前词`w_t`依赖于其前面的词`w_{1:(t−1)}`。而在连续词袋模型和Skip-Gram模型中，当前词`w_t`**依赖于其前后的词**。
 - 模型结构：
    - 连续词袋模型（CBOW）
        ```
        给定一个词w_t的其上下文：
        c_t = w_{t-n},···,w_{t-1}, w_{t+1},···,w_{t+n}
        
        和标准语言模型不同，上下文c_t可以同时取左右两边的n个词。
        
        连续词袋模型CBOW是该词w_t出现的条件概率为:
        P(w_t|c_t) = softmax(v'_{w_t}^T c_t)
                   = exp(v'_{w_t}^T c_t) / ( sum_{w'∈V} (exp(v'_{w_t}^T c_t)) )
        
        其中，c_t表示上下文信息。
        ```
        - 在连续词袋模型中，就**直接把隐藏层去掉,大大减少了计算量**,提高了计算速度,然后用更多的数据来训练模型,最后的效果也不错
        - 给定一个训练文本训练`w1,···,wT`，连续词袋模型的目标函数为：
            ```
            L_θ = - 1/T sum_{t=1}^T log p(w_t|c_t)
            ```
        - 即通过当前词的两边词（上下文信息）来计算当前词的后验概率，没有隐藏层，直接进行计算
    - Skip-Gram模型
        - Skip-Gram模型给定一个词`w_t`，**预测词汇表中每个词出现在其上下文中的概率**。
            ```
            P(w_{t+j}|w_t) = softmax( v_{w_t}^T  v'_{w_{t+j}} )
                           = exp( v_{w_t}^T  v'_{w_{t+j}} ) / ( sum_{w'∈V} exp( v_{w_t}^T  v'_w' ) )
            
            其中，v_w表示词w在输入词嵌入矩阵中的词向量，
            v'_w表示词w在输出词嵌入矩阵中的词向量。
            ```
        - Skip-Gram模型没有隐藏层，`h_t`直接等于词嵌入`v_{w_t}`。
        - 给定一个训练文本训练`w1,···,wT`，Skip-Gram模型的目标函数为：
            ```
            L_θ = - 1/T sum_{t=1}^T sum_{-n<=j<=n,j!=0} log P(w_{t+j}|w_t)
            ```
 - 训练方法：
    - 在Word2Vec中，连续词袋模型和Skip-Gram模型都可以通过两种训练方法（层次化`softmax`和负采样）来加速训练。
        - 层次化`softmax`，。在Word2Vec中采样了Huffman树来进行词汇表的层次化
        - 负采样：该方法可以看成是噪声对比估计方法的一个简化版本。
            ```
            给定上下文信息c，对于词汇表中每一个词w，
            w来自于真实分布的概率为：
            P(y=1|w,c) = exp(s(w, c; θ)) / ( exp(s(w, c; θ)) + 1 )
                       = 1 / ( 1 + exp(−s(w, c; θ)) )
                       = σ(s(w, c; θ))
            
            其中，σ为logistic函数，s(w, c; θ)为模型得分
            σ = 1 / (1+exp(−x))
            
            给定一个训练文本序列w1,···,wT，在位置t时，连续词袋模型和Skip-Gram模型的s(w_t, c_t; θ)定义如下：
                - 在连续词袋模型中，预测目标w为当前词w_t，   c为上下文词
                    s(wt, ct; θ) = v'_{w_t}^T sum_{-n<=j<=n,j!=0} v_{w_{t+j}}
                - 在Skip-Gram模型中，预测目标w为上下文词，  c为当前词w_t
                    s(wt, ct; θ) = v_{w_t}^T v'_{w_{t+j}} , −n ≤ j ≤ n, j ̸= 0
            ```
            - 使用负采样方法进行训练时，对于每个正例(w_t, c_t)，用噪声分布Q(w)中随机采样k个负例。k的取值由数据大小决定，
            通常小规模数据k的取值范围在5∼20，而大规模数据k可以非常小2∼5
            - 和噪声对比估计类似，负采样方法的目标函数也是一个二分类问题
            - 通过logistic回归来区分目标词w是来自真实分布还是噪声分布`Q(w)`
            - 和噪声对比估计不同的是，噪声分布`Q(w)`只是用来采样，而不参与计算。因此，噪声对比估计可以近似语言模型，而负采样不可以。
    - Word2Vec加速技巧：
        - **删除隐藏层**，得到上下文c的表示后，直接输入到`softmax`分类器来预测输出。
        也就是说，整个网络的参数只有两个词嵌入表：输入词嵌入表和输出词嵌入表；
        - 使用**层次化`softmax`或负采样**进行加速训练
        - **去除低频词**。出现次数小于一个预设值minCount的词直接去除。
        - **对高频词进行降采样**：
            ```
            根据下面公式算出的概率P_{discard}(w_t)来跳过词w_t
            这样可以节省时间而且可以提高非高频词的准确度。
            
            P_{discard}(w_t) = 1 - sqrt( m / U(w_t) )
            
            其中，m为设定好的阈值，一般取10−5，U(w_t)为w_t的一元语言模型频率。
            也就是说，对于词，U(w_t)越大（高频词的较大），P_{discard}(w_t)就越大，用来以该概率来跳过高频词
            ```
        - **动态上下文窗口大小**。指定一个最大窗口大小值N，对于每个词，从\[1,N]中随机选取一个值n来作为本次的上下文窗口大小，
        从当前词左右各选取n个词。这样上下文更侧重于邻近词。
        - **噪声分布使用一元语言模型**`U(x)`的`U(x)^{3/4} / Z`，Z为归一化因子。**相当于对高频词进行降采样，对低频词进行上采样**。
 - 从连续词袋模型和Skip-Gram模型的定义可以看出，**对于上下文相似的词，其向量也会相似**。这和分布式假设的定义十分吻合。
 **分布式假设的定义为如果两个词的上下文分布相似，那么这两个词的词义也是相似的**。

**总结：**
 - 词嵌入，即词的分布式表示，是NN来NLP的前提和关键因素。
 - NNLM需要给词汇表中的每一个词都赋予一个概率，即一个类别数为|V|的多类分类问题，类别数远大于一般的机器学习任务。因此在进行`softmax`归一化时计算代价很高。
    - Bengio \[2008]提出了利用重要性采样来加速`softmax`的计算
    - Mnih \[2013]提出了噪声对比估计来计算非归一化的条件概率。
    - Morin and Bengio \[2005]最早使用了层次化`softmax`函数来近似扁平的`softmax`函数。
 - 但是通过NNLM来预训练词嵌入由两个不足：
    - 一是即使使用改进的NNLM，其训练也需要大量的计算资源训练，训练时间非常长。
    - 二是NNLM的优化目标是降低语言模型的困惑度，和词嵌入的好坏并不是强相关关系。
 - Mikolov \[2013a]提出了两种非常简化的模型：连续词袋模型和Skip-Gram模型。其核心思想是当语料足够大时，简单的模型也能得到较好的词表示。
 - 大量的词是有多种义项，在不同上下文中，词的意义也不同，如果只赋予每一个词一个词向量或多个词向量，会出现问题。
 可参考基于上下文的词嵌入模型\[Huang et al., 2012, Neelakantan et al., 2014]。


## GloVe模型
参考博客：[理解GloVe模型](https://blog.csdn.net/coderTC/article/details/73864097)

**概述：**
 - 模型目标：进行词的向量化表示，使得向量之间尽可能多地蕴含语义和语法的信息。
 - 输入：语料库
 - 输出：词向量
 - 方法概述：首先基于语料库构建**词的共现矩阵**，然后基于共现矩阵和GloVe模型学习词向量。

**统计词共现矩阵:**
 - 设共现矩阵为X，其元素为`X_{i,j}`。 
 - `X_{i,j}`的意义为：在整个语料库中，单词i和单词j共同出现在一个窗口中的次数。 
 - 例子：
    ```
    语料库：
    i love you but you love him i am sad
    
    这个小小的语料库只有1个句子，涉及到7个单词：i、love、you、but、him、am、sad。 
    如果我们采用一个窗口宽度为5（左右长度都为2）的统计窗口，那么就有以下窗口内容：
    ```
    窗口标号 | 中心词 | 窗口内容
    -|-|-
    0 | i | i love you |
    1 | love | i love you but |
    2 | you | i love you but you |
    3 | but | love you but you love |
    4 | you | you but you love him |
    5 | love | but you love him i |
    6 | him | you love him i am |
    7 | i | love him i am sad |
    8 | am | him i am sad |
    9 | sad | i am sad |
    ```
    窗口0、1长度小于5是因为中心词左侧内容少于2个，同理窗口8、9长度也小于5。
    以窗口5为例说明如何构造共现矩阵：
    中心词为love，语境词为but、you、him、i；
    使用窗口将整个语料库遍历一遍，即可得到共现矩阵X
    
    GloVe模型没有使用神经网络的方法
    ```

**Glove和skip-gram、CBOW模型对比：**
 - CBOW/Skip-Gram 是一个local context window的方法，比如使用负采样来训练，缺乏了整体的词和词的关系，负样本采用sample的方式会缺失词的关系信息。
 - 另外，直接训练Skip-Gram类型的算法，很容易使得高曝光词汇得到过多的权重
 - Global Vector融合了矩阵分解Latent Semantic Analysis (LSA)的全局统计信息和local context window优势。融入全局的先验统计信息，可以加快模型的训练速度，又可以控制词的相对权重。
 - CBOW/Skip-Gram每次都是用一个窗口中的信息更新出词向量，但是Glove则是用了全局的信息（共现矩阵），也就是多个窗口进行更新


## word2vec数学原理
是一个开源工具包，用来获取词的向量


## Beam Search算法/贪心算法/维特比算法
参考知乎：
 - [seq2seq中Beam search~贪心与维特比](https://www.zhihu.com/search?type=content&q=%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95%20%E7%BB%B4%E7%89%B9%E6%AF%94%E7%AE%97%E6%B3%95%20%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%20beam%20search)
 - [seq2seq中Beam search~Bs算法](https://zhuanlan.zhihu.com/p/42006789)

**贪心算法：**
 - 如果target sequence词汇表的大小为`|V_E|`的话，对于解码器的N步输出，他的搜索空间`|V_E|^N`。随着N的增大，那这个效率会非常低。所以我们才想要通过一些算法去找出使得概率最大的输出序列。
 - 贪心算法每一步都会计算当前步的最优解（贪心算法的思想，每一步都选择最大的概率），最后的输出序列不一定是全局的最优解。

**维特比算法：**
 - 目标是找出使`P(e1,e2,...,en|F)`最大的序列`e1,e2,...,en`。
 - 在HMM中使用了维特比算法
 - 算法思想：
    - 用动态规划的思想来求解概率最大的路径（最优路径）
    - 这个最终的最优路径就是我们想要得到的最终的输出序列。
    - 简单的说我们只需从第1步开始，递推地计算在第t步输出单词为e的各条部分路径的最大概率，直至得到最后一步输出单词e的各条路径的最大概率。
    - -------------------------- 分割线 --------------------------
    - 在第t步，计算当前时间步所有状态的最优路径
        ```
               第一步     第二步     第三步
                 e1        e2       e3
            |--> a         a         a
        F-->|--> b 交叉连接  b 交叉连接  b
            |--> c         c         c
        ```
    在第一步，假设：
    
        ```
        p(F-->a)=0.5
        p(F-->b)=0.3
        p(F-->c)=0.2
        ```
    第二步，假设：
    
        ```
        p(a-->a)=0.3
        p(a-->b)=0.4
        p(a-->c)=0.3
        
        p(b-->a)=0.4
        p(b-->b)=0.2
        p(b-->c)=0.4
        
        p(c-->a)=0.5
        p(c-->b)=0.2
        p(c-->c)=0.3
        ```
    则，在第二步，到达各个状态的最佳路径是：
    
        ```
        对于状态a：
            - 从第一步中三个状态分别到达第二步中的a状态
                F --> a --> a， p=0.5×0.3=0.15  最大，最优路径
                F --> b --> a， p=0.3×0.4=0.12
                F --> c --> a， p=0.2×0.5=0.10
            - 所以到达a的最优路径是 'F->a->a'
        
        对于状态b：
            - 从第一步中的三个状态分别到达第二步中的b状态
                F --> a --> b， p=0.5×0.4=0.20  最大，最优路径
                F --> b --> b， p=0.3×0.2=0.06
                F --> c --> b， p=0.2×0.2=0.04
            - 所以到达b的最优路径是 'F->a->b'
        
        对于状态c：
            - 从第一步中的三个状态分别到达第二步中的c状态
                F --> a --> c， p=0.5×0.3=0.15  最大，最优路径
                F --> b --> c， p=0.3×0.4=0.12
                F --> c --> c， p=0.2×0.3=0.06
            - 所以到达c的最优路径是 'F->a->c'
        ```
    第三步，假设：
    
        ```
        p(a-->a)=0.4
        p(a-->b)=0.4
        p(a-->c)=0.2
        
        p(b-->a)=0.3
        p(b-->b)=0.4
        p(b-->c)=0.3
        
        p(c-->a)=0.7
        p(c-->b)=0.1
        p(c-->c)=0.2
        ```
    则，在第三步，到达各个状态的最佳路径是：
    
        ```
        在第二步中，已经确定，
            - 到达a的最优路径是 'F->a->a'  0.5×0.3
            - 到达b的最优路径是 'F->a->b'  0.5×0.4
            - 到达c的最优路径是 'F->a->c'  0.5×0.3
        
        则，在第三步中，从第二步的3个状态，分别到达第三步中的[a,b,c]状态的分析如下：
        对于状态a：
            - 从第二步中的三个状态分别到达第三步中的a状态，
                F --> a --> a --> a， p=0.5×0.3×0.4=0.06  
                F --> a --> b --> a， p=0.5×0.4×0.3=0.06
                F --> a --> c --> a， p=0.5×0.3×0.7=0.105  最大，最优路径
            - 所以到达a的最优路径是 'F->a->c->a'，值0.105
        
        对于状态b：
            - 从第二步中的三个状态分别到达第三步中的b状态
                F --> a --> a --> b， p=0.5×0.3×0.4=0.06   
                F --> a --> b --> b， p=0.5×0.4×0.4=0.08   最大，最优路径
                F --> a --> c --> b， p=0.5×0.3×0.1=0.015
            - 所以到达b的最优路径是 'F->a->b->b'，值0.08
        
        对于状态c：
            - 从第二步中的三个状态分别到达第三步中的c状态
                F --> a --> a --> c， p=0.5×0.3×0.2=0.03   
                F --> a --> b --> c， p=0.5×0.4×0.3=0.06   最大，最优路径
                F --> a --> c --> c， p=0.5×0.3×0.2=0.03
            - 所以到达a的最优路径是 'F->a->b->c'，值0.06
        ```
    进行对比`0.105 > 0.08 > 0.06`，故最优路径是`F->a->c->a`
    - 根据上边的计算过程可知，维特比算法求出的最佳路径是针对每个具体的状态来说的，采用了动态规划的思想，
    比如最佳路径是`F->a->c->a`，子路径`F->a->c`表示第二步到达c状态时的最优路径，并不是整个第二步的最优路径。
 - 维特比算法的复杂度：
    - 从上面过程的计算复杂度为`V*N`，其中，V表示每一步的状态的个数，N表示步数，也就是说在每一步都需要计算N个状态的概率。
    在当前步中计算某个状态时，都需要从上一步的V个状态中去遍历，故计算复杂度是`V*N*V`。
    - 空间复杂度是`V*N`，表示每一步的每一个状态的概率（并记录达到这个最优概率的父节点）。
 - 优缺点：
    - 维特比算法还是很不错的，能够得到最优的值，但是如果target sequence词汇表特别大的话，效率还是不高，当然target sequence词汇表很小的时候，维特比算法会是一个很不错的选择
    - 但是通常我们的target sequence词汇表都很大。所以就有了Beam search算法，他通过舍弃一些精度来提高效率。
    - 得到的是**全局最优解**

**Beam Search算法：**
 - 目标还是找出使`P(e1,e2,...,en|F)`最大的序列`e1,e2,...,en`。
 - Beam search方法中有一个关键的参数Beam Size B，这个B是远远小于V的，即`B << V`，V表示词汇表的大小，
 维特比算法需要填充一个`V*N`的表格，Beam Search需要填充一个`B*V`的表格，直观的来看beam search比Viterbi算法效率高很多，因为`B << V`。
 - 具体过程如下：
 
    ```
    假设B=2，即每一步都选择出概率值最大的两个作为该步的输出。
    
    第一步，
        - 对下列概率进行降序排列，保留最大的两个：
            p(F-->a)=0.5
            p(F-->b)=0.3
            p(F-->c)=0.2
        - 输出：
            p(F-->a)=0.5
            p(F-->b)=0.3
    第二步，
        对于状态a：
            - 从第一步中2个状态分别到达第二步中的a状态
                F --> a --> a， p=0.5×0.3=0.15  最大，最优路径
                F --> b --> a， p=0.3×0.4=0.12
            - 所以到达a的最优路径是 'F->a->a'
        
        对于状态b：
            - 从第一步中的2个状态分别到达第二步中的b状态
                F --> a --> b， p=0.5×0.4=0.20  最大，最优路径
                F --> b --> b， p=0.3×0.2=0.06
            - 所以到达b的最优路径是 'F->a->b'
        
        对于状态c：
            - 从第一步中的2个状态分别到达第二步中的c状态
                F --> a --> c， p=0.5×0.3=0.15  最大，最优路径
                F --> b --> c， p=0.3×0.4=0.12
            - 所以到达c的最优路径是 'F->a->c'
    
        进行排序，输出最大的两个：
            F->a->b， 值为0.20
            F->a->c， 值为0.15
    
    第三步，
        对于状态a：
            - 从第二步中的2个状态分别到达第三步中的a状态，
                F --> a --> b --> a， p=0.5×0.4×0.3=0.06
                F --> a --> c --> a， p=0.5×0.3×0.7=0.105  最大，最优路径
            - 所以到达a的最优路径是 'F->a->c->a'，值0.105
        
        对于状态b：
            - 从第二步中的2个状态分别到达第三步中的b状态
                F --> a --> b --> b， p=0.5×0.4×0.4=0.08   最大，最优路径
                F --> a --> c --> b， p=0.5×0.3×0.1=0.015
            - 所以到达b的最优路径是 'F->a->b->b'，值0.08
        
        对于状态c：
            - 从第二步中的2个状态分别到达第三步中的c状态
                F --> a --> b --> c， p=0.5×0.4×0.3=0.06   最大，最优路径
                F --> a --> c --> c， p=0.5×0.3×0.2=0.03
            - 所以到达a的最优路径是 'F->a->b->c'，值0.06
    
        进行排序，输出最大的两个：
            F->a->c->a， 值为0.105   最大，最优路径
            F->a->b->b， 值为0.08
    ```
 - Beam Search算法的复杂度：
    - 计算复杂度`O(B×N×log(V))`，我们是按列进行填写的，所以需要计算`B×N`个，我们对temp进行排序的是`B×log(V)`，
    所以是`B×N + B×V×log(V)`，所以每一列的计算复杂度是`O(B×V×log(V))`，那总共有N列，所以计算复杂度为`O(B×V×N×log(V))`；
    - 空间复杂度就是表格中需要填的元素个数，所以空间复杂度为`O(B×N)`
 - 可以看出，beam search算法还是很不错的，他得到的结果是**近似的最优解**
 - 如果词汇表特别大的话，计算复杂度也不会太大，比维特比算法和贪心算法效率高很多。



## 前后向算法


## 序列生成算法




































































## BERT-style模型

BERT/spanBERT/RoBERT/Albert/...




#### Transformer

recurrent attention






Attention机制允许模型依赖，不用考虑输入输出序列中的距离

Transformer用一个attention机制去生成输入输出的全局依赖表示，完全取代了循环网络去生成

Transformer将依赖操作的数量限定在一个常数，尽管因为平均attention-weighted position牺牲了有效性，这可以使用Multi-Head Attention来抵消



模型架构

Transformer之前，大部分的神经序列转换模型有一个encoder-decoder结构，encoder将输入符号序列`x=(x_1,x_2,...,x_n)`映射到一个连续的序列表示`z=(z_1,z_2,...,z_n)`，
然后给定z，使用decoder去生成输出符号序列`y=(y_1,y2,...,y_m)`，在一个时间只生成一个元素

在每一个step，模型是auto-regressive，在生成下一个符号时将之前生成的符号作为额外的输入

Transformer基于上述这个总体架构，encoder和decoder使用堆叠的self-attention和point-wise的全连接层







1.Transformer的decoder中，解码器的上一个输出做为Q还是解码器所有之前的输出作为Q？

2.在encoder之前，为了捕捉顺序序列的能力，加上了位置编码，可以直接使用一层RNN来捕捉？

Q,K,V是经过线性变换之后，才进行切分


为了保持自回归特性，我们需要防止解码器中的信息流向左流动

```
>>> nn.softmax(torch.tensor([1,2,-10000000000]).float(), dim=0)
tensor([0.2689, 0.7311, 0.0000])
```



变化的学习率：这相当于在前warmup_steps个训练步骤中线性地增加学习率，然后与步骤数的平方反比成比例地降低学习率。

warmup_steps是固定的值，所以`warmup_steps^{-1.5}`也是固定值，`step_num·warmup_steps^{-1.5}`中step_num在线性增加，值越来越大



进行缩放的原因：
 - 矩阵-向量乘法各种模型里到处都是啊，也不会造成梯度流动的问题。dot-product attention 的问题是他要对点积过后的东西做 softmax，各个分量是互相影响的（而不像 tanh 之类的函数，各个分量各算各的）。导致的结果就是：向量维度越高，点积的结果范围越大，越有可能出现最大值比其他值大很多的情况，导致 softmax 的结果接近 one-hot（可以计算一下 softmax(np.random.random(10)) 和 softmax(100 * np.random.random(10))，后者概率质量集中到某个维度的现象很明显


In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation：
 - 那两个embedding其实是词向量的正转换（onehot-词向量）和逆转换
 - 原语言和目标语言混用了词表



参考博客：
 - [详解Transformer](https://zhuanlan.zhihu.com/p/48508221)
 - [10分钟带你深入理解Transformer原理及实现](https://zhuanlan.zhihu.com/p/80986272)
 - [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)
 - [transformer中为什么使用不同的K和Q，为什么不能使用同一个值？](https://www.zhihu.com/question/319339652/answer/1012823289)


















































#### BERT

Bidirectional Encoder Representations from Transformers

为什么ELMo是feature-based？


预训练深度双向表示从无标签的文本中，在所有层中联合左边和右边的文本

相关工作：
 - Unsupervised feature-based -- ELMo
 - Unsupervised fine-tuning -- BERT
 - Transfer Learning from Supervised Data

之前标准的语言模型是单向的，

OpenAI GPT:
 - a left-to-right架构，在transformer的self-attention层，每一个token仅仅attend to之前的tokens
 - 这个限制对句子水平的任务而言是次最优的，当在token水平的任务进行精调时是毁灭性的，例如SQuAD任务
 - 使用一个浅层的连接去单独训练left-to-right和right-to-left LMs，来抽取context-sensitive features
 - 每个token的上下文表示是left-to-right表示和right-to-left表示的连接


BERT改进：
 - MLM：masked LM
    - BERT为了解决之前语言模型中单向的限制，提出了一个新的预训练目标：masked language model (MLM)
    - MLM随机mask一些输入中的token，目标是仅仅基于它的上下文来预测被masked的word的原始的词汇id
    - 不像left-to-right的语言模型预训练，MLM目标允许表示去融合left和right的上下文，允许我们去预训练一个深度双向的Transformer
    - 该双向性是BERT最重要的贡献
 - Next Sentence Prediction
    - 联合预训练text-pair表示


[The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)


BERT：
 - 模型架构：
    - 是一个基于原始Transformer的，多层·双向·Transformer encoder
    - BERT base  (L=12, H=768, A=12, Total Parameters=110M)
    - BERT large (L=24, H=1024, A=16, Total Parameters=340M)
    - 相比GPT来说，BERT Transformer使用双向self-attention，而GPT Transformer使用受约束self-attention，也就是每一个token仅仅attend to它左边的context
    - 在一些文献中bidirectional Transformer常常被称为Transformer encoder，而left-context-only的版本被称为Transformer decoder，因为它能被用来做文本生成
 - 输入/输出表示：
    - xx
 - 该框架分为两步：
    - pre-training
        - 和ELMo、GPT的left-to-right或者right-to-left LMs的预训练不同，BERT采用两个无监督的任务去预训练
        - 任务1： Masked LM
            - 深度双向模型完全比一个left-to-right模型或者一个left-to-right和一个right-to-left模型的浅层连接
            - 因为双向的条件LM将会使每一个词间接地看到它自己，为了训练一个深度双向的表示，需要随机mask一定百分比的输入词，然后预测那些被masked掉的词，该过程就是MLM
            - 对于每个序列，随机mask掉15%的wordpiece tokens，和降噪自编码不同，我们仅仅预测被masked的word，而不是重构整个句子
            - 虽然上述过程允许我们获得一个双向预训练模型，缺点是这使得我们创造了一个pre-training和fine-tuning的不匹配，因为`[MASK]` token在fine-tuning时没有出现
            - 为了减轻上述的影响，我们并不总是使用`[MASK]`取代masked token
            - 具体过程：
                - 随机选择15%的token位置来预测
                - 加入第i个token被选中，我们取代第i个token：
                    - 在80%的时间里使用`[MASK]`
                    - 在10%的时间里使用随机token
                    - 在10%的时间里token不变
                - 然后第i个输入最后的隐藏向量`T_i`被用来预测原始的token，使用交叉熵损失函数
        - 任务2： Next Sentence Prediction (NSP)
            - 下游任务QA和NLI是基于理解两个句子的关系的基础上的，这种关系没有被LM所捕获
            - 对于每一个预训练的例子在选择句子A和句子B的时候：
                - 在50%的时间里，句子B是真实的下一句
                - 在50%的时间里，句子B是语料中的一个随机的句子
    - fine-tuning
        - BERT用一个self-attention机制来同时对文本对进行编码，在两个句子间做双向cross attention
        - BERT large在小数据集上表现比BERT base好很多
        - **在fine-tuning SQuAD之前，先在TriviaQA上fine-tuning（将该数据集进行格式化，取篇章的前400个tokens，取出来的片段包含正确答案的），以进行适当的数据增强**
        - 对于SQuAD2.0无答案的情况，如果answer span的开始和结束位置落在`[CLS]`中，当作无答案的情况：
            
            ```
            C为[CLS] token, T_i为正常token，S为开始向量，属于R^H，H为隐状态的维度，E为结束向量，属于R^H
            
            通过计算T_i和S的点乘，然后经过softmax over all words，值最大的位置即为开始位置，同理计算出结束位置
            
            从位置i到位置j的候选span的分数为：
            S·T_i + E·T_j，  其中j>=i
            
            无答案的span分数（即落在[CLS]中）：
            s_{null} = S·C+E·C
            有答案的span分数的最大值：
            s_{i,j} = max_{j>=i} (S·T_i + E·T_j)
            
            通过比较下式来预测非空答案：
            s_{i,j} > s_{null} + τ
            其中阈值τ通过在dev数据集上最大化F1值来选择
            
            在这个版本的数据集上没有使用TriviaQA进行精调
            ```
        - xx


















































































## ELMO模型


## word2vec与BERT的区别


## 一些问题
**线性插值法和Attention机制：**



**word2vec、glove、cove、fastext以及elmo对于知识表达有什么优劣？**
 - word2vec：
    - 一般来说，基于Skip-gram训练出的词向量更细致，尤其是语料库较小或者有较多低频词的时候，使用Skip-gram更为合适。
    - Word2vec是一个“线性”的语言模型，训练出来的词向量支持一些线性的语义运算，如经典的“皇帝-皇后=男-女”。
    - 如果遇到一个生僻词，一般都能根据上下文大概猜出这个生僻词的含义，Word2vec其实就是基于这种思想。
    - 但是Word2vec并**没有考虑到词序信息以及全局的统计信息**等。
 - GloVe：
    - 针对Word2vec只考虑词局部信息的问题，GloVe尽可能的利用词汇的共现（co-occurrence）信息，构建了一个词汇的共现矩阵，并对这个共现矩阵进行降维。
    - GloVe词向量直译为全局的词向量表示，跟word2vec词向量一样本质上是基于词共现矩阵来进行处理的。
    - glove利用了全局信息，使其在训练时收敛更快，训练周期较Word2vec较短且效果更好
    - 基于词共现矩阵收集词共现信息。
    - 然后GloVe 模型损失函数：计算余弦相似度，语义类比，我们可以利用词汇之间的余弦相似性计算空格处到底填什么单词。
    - Glove和word2vec在并行化上有一些不同，即GloVe更容易并行化，所以对于较大的训练数据，GloVe更快
 - fastText：
    - fastText其实是Word2vec的一个拓展，其核心是n-gram，作者在Word2vec的基础上加入了n-gram信息，将Word2vec中中心词的向量转化成中心词n-gram的向量表示。
    - 由于加入了n-gram信息，fastText可以一定程度上解决OOV问题。fastText比较适用于形态丰富的语言，如俄语、土耳其语、法语等。中文上的效果可能没有上述语言来得好。
    - FastText模型构架与Word2Vector中连续词袋CBOW模型有点类似，不同之处在于，FastText预测标签，而CBOW预测中间词。FastText是把句子中所有的词向量进行平均，
    然后直接连接全连接层（softmax）层，加入一些n-gram特征来捕获局部序列信息，文本分类不必做过多的非线性转换、特征组合即可捕获很多分类信息
 - ELMo：
    - 无论是Word2vec、GloVe还是fastText，其本质都是一个“静态”的词向量，即训练完成后，一个词对应的向量是静止不变的。而自然语言中有很多的多义词，这恰恰是人类语言高效泛用的体现。
    - ELMo与上述词向量不同，它是一个“动态”的词向量。何为“动态”呢？
        - ELMo认为一个词的embedding可能有多种形式（一词多意，不同的上下文可能不同），**能解决多义词问题**。
        - 同时ELMo使用了一层CNN和两层LSTM，模型结构更深，能捕捉到更多的信息。
        - 而且不同于Cove词向量，ELMo提出可以根据**不同的下游任务进行线性组合，最大化的利用不同层抽取出的特征**，使得ELMo词向量有很强的泛用性。
    - ELMo与word2vec最大的不同：即词向量不是一成不变的，而是根据上下文而随时变化，这与word2vec或者glove具有很大的区别













































