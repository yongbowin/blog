# 语言模型

> 目前主要采用的是n元语法模型，这种模型构建简单、直接，但同时也因为数据缺乏 而必须采取平滑（smoothing）算法

**参考文献：**
 - 《统计自然语言处理》 by 宗成庆
 - 《深度学习》(花书) by Bengio
 - 《神经网络与深度学习》 by 邱锡鹏 第14章


**一般概念：**
 - 词的局部表示就是one-hot向量表示，即词袋模型(Bag-of-Words, BOW)，即将文本看成是词的集合，不考虑词序信息。又叫做向量空间模型(Vector Space Model, VSM)
 - 将高维的局部表示向量空间`R^{|V|}`映射到一个非常低维的空间`R^d`，V为词表，。这个低维空间中的表示就是**分布式表示**
 - 对于词的分布式表示（即低维稠密向量表示），我们经常叫做词嵌入（Word Embeddings）
 - 在机器学习中，**嵌入**通常指将一个度量空间X中的对象映射到另一个低维的度量空间Y中，并进行可能保持不同对象之间拓扑关系。
 - 在one-hot向量空间中，每个词都位于坐标轴上，每个坐标轴上一个词；而在低维的嵌入空间中，每个词都不在坐标轴上，词之间可以计算相似度。

## 统计语言模型
**定义：**
 - 统计语言模型把语言（词/字/字符的序列）看作一个随机事件，并赋予相应的概率来描述其属于某种语言集合的可能性。
 - 给定一个词汇集合V，对于一个由V中的词构成的序列`S=⟨w1,···,wT⟩ ∈ V^n`，统计语言模型赋予这个序列一个概率`P(S)`，来衡量S符合自然语言的语法和语义规则的置信度。

简单地说，统计语言模型就是用来**计算一个句子的概率的模型**。统计语言模型可以估计出自然语言中每个句子出现的可能性，而不是简单的判断该句子是否符合文法。

**一些性质：**
 - 语言模型的两个基本功能是：
    - 判断一段文本是否符合一种语言的语法和语义规则
    - 生成符合一种语言语法或语义规则的文本
 - [多项分布](https://baike.baidu.com/item/%E5%A4%9A%E9%A1%B9%E5%88%86%E5%B8%83/6548502?fr=aladdin)。
 多项式分布是二项式分布的推广。二项分布的典型例子是扔硬币，硬币正面朝上概率为p,重复扔n次硬币，k次为正面的概率即为一个二项分布概率。把二项分布公式推广至多种状态，就得到了多项分布。
 - 在传统的统计语言模型中，一般**假设语言是服从多项分布**，并**利用最大似然估计来求解多项分布的参数**(也就是求解前n-1个词的条件下，第n个词的概率)。
 - **最大似然估计等价于频率估计**


**N元语法：**
 - 一个语言模型**通常构建为字符串s的概率分布`p(s)`，试图反应的是字符串s作为一个句子出现的频率。**
 - 与语言学中不同，语言模型与句子是否合乎语法是没有关系的，即使一个句子完全合乎语法逻辑，我们仍然可以认为它出现的概 率接近为零
 - 句子`s=w_{1}w_{2}...w_{l}`，其概率计算公式可以表示为：
     ```
     p(s) = p(w_1)p(w_2|w_1)p(w_3|w_1w_2)...p(w_l|w_1...w_{l-1})
          = \Pi_{i=1}^l p(w_i|w_1...w_{i-1})
     ```
 - n元语法（或n元文法，n-gram），用来解决当前词的历史词过多时计算上述公式参数过多的问题
    - 产生第i个词的概率是由已经产生的i-1个词决定的，把前边的i-1个词成为第i个词的历史
    - 随着历史长度的增加，不同的历史数目按指数级增长，这使我们几乎不可能从训练数据中正确地估计出这些参数，实际上，绝大多数历史根本就不可能在训练数据中出现。
    - 为了解决这个问题，可以将历史w1w2…wi-1按照某个法则映射到等价类E(w1w2…wi-1)，而等价类的数目远远小于不同历史的数目。
    - 如果假定`p(w_i|w_1,w_2,...,w_{i-1}) = p(w_i|E)`，那么，自由参数的数目就会大大地减少
    - 一种比较实际的做法是，将两个历史映射到同一个等价类，当且仅当这两个历史**最近的`n-1`个词相同**
    - 满足上述条件的语言模型称为n元语法
    - 通常情况下，n的取值不能太大，否则，等价类太多，自由参数过多的问题仍然存在。
        - 在实际应用中，取n=3的情况较多（即当前词仅与它之前的前两个词相关，3-gram）。
        - 当n=1时，即出现在第i位上的词w_i独立于历史时（不与之前的任何词相关），一元文法被记作unigram，或uni-gram，或monogram
        - 当n=2时，即出现在第i位上的词w_i仅与它前面的一个历史词w_{i-1}有关，**二元文法模型被称为一阶马尔可夫链**(Markov chain)，记作bigram或bi-gram
        - 当n=3时，即出现在第i位上的词w_i仅与它前面的两个历史词w_{i-2}w_{i-1}有关，**三元文法模型被称为二阶马尔可夫链**，记作trigram或tri-gram
    - 以二元语法模型为例，根据前面的解释，我们可以近似地认为，一个词的概率只依赖于它前面的一个词，那么概率计算公式近似为：
        ```
        p(s) = \Pi_{i=1}^l p(w_i|w_1...w_{i-1})
             = \Pi_{i=1}^l p(w_i|w_{i-1})
        ```
        - 为了让上式对于i=1有意义，我们在句子开头加上一个句首标记<BOS>，即假设w_0就是<BOS>
        - 为了使得所有的字符串的概率之和`sum_{s}p(s)`等于1，需要在句子结尾再放一个句尾标记<EOS>，并且使之包含在上边等式的乘积中，
        (如果不做这样的处理，所有给定长度的字符串的概率和为1，而所有字符串的概率和为无穷大，因为不知道什么时候是句子的结束，当知道句子的长度时，在知道在哪结束句子)
        - 为了估计条件概率`p(w_i|w_{i-1})`，可以简单地计算二元语法`w_{i-1}w_i`在某一文本中**出现的频率**，然后归一化。
        如果用`c(w_{i-1}w_i)`表示二元语法`w_{i-1}w_i`**在给定文本中的出现次数**，我们可以采用下面的计算公式:
            ```
            p(w_i|w_{i-1}) = c(w_{i-1}w_i) / sum_{w_i}c(w_{i-1}w_i)
            ```
        分母`sum_{w_i}c(w_{i-1}w_i)`表示计算历史`c(w_{i-n+1}^{i-1})`的数目，具体见下边例子
        - 用于构建语言模型的文本称为训练语料。对于n元语法模型，使用的训练语料的规模一般要有几百万个词。上式用于估计`p(w_i|w_{i-1})`的方法称为`p(w_i|w_{i-1})`的**最大似然估计**
        - 计算例子：
            ```
            假设语料S由下边的3个句子构成：
            ("BROWN READ HOLY BIBLE", 
             "MARK READ A TEXT BOOK", 
             "HE READ A BOOK BY DAVID")
             
            用计算最大似然估计的方法计算概率 p(BROWN READ A BOOK)：
            p(BROWN|<BOS>) = c(<BOS> BROWN) / sum_{w}c(<BOS> w) = 1/3
            p(READ|BROWN) = c(BROWN READ) / sum_{w}c(BROWN w) = 1/1
            p(A|READ) = c(READ A) / sum_{w}c(READ w) = 2/3
            p(BOOK|A) = c(A BOOK) / sum_{w}c(A w) = 1/2
            p(<EOS>|BOOK) = c(BOOK <EOS>) / sum_{w}c(BOOK w) = 1/2
            
            因此：
            p(BROWN READ A BOOK) = p(BROWN|<BOS>) ×
                                   p(READ|BROWN) ×
                                   p(A|READ) ×
                                   p(BOOK|A) ×
                                   p(<EOS>|BOOK)
                                 = 1/3 × 1/1 × 2/3 × 1/2 × 1/2
                                 = 0.06
            ```

**n-gram与马尔可夫链的关系：**
 - 二元文法模型被称为一阶马尔可夫链，三元文法模型被称为二阶马尔可夫链

**困惑度：**
 - 困惑度(Perplexity)可以用来衡量一个分布的不确定性，我们也可以用困惑度来衡量两个分布之间差异
 - 对于一个未知的数据分布`P_{data}(X)`，和一个模型分布`P_{model}(X)`，我们从`P_{data}(X)`中采样出一组测试样本`x1,···,xN`，模型分布`P_{model}(X)`的困惑度为：
     ```
     2^{ H(P'_{data}, P_{model}) } = 2^{-1/N sum_{i=1}^N log_{2}P_{model}(x_i)}
     
     其中H(P'_{data}, P_{model})为样本的经验分布P'_{data}和模型分布P_{model}之间的交叉熵，  
     也是所有样本上的负对数似然函数
     ```
 - 模型的困惑度**可以衡量模型估计分布与样本经验分布之间的契合程度**，困惑度越低则两个分布越接近。因此，模型`P_{model}`的好坏可以用困惑度来评价。

**语言模型性能评价：**
 - 一个好的语言模型应该使得测试集合中的**句子的联合概率尽可能高**
 - 语言模型最直接的评价就是测试集中所有句子的联合概率，一个好的语言模型在测试集上的概率越高越好。但是联合概率的缺点是一般都比较小，并且和句子长度有关。越长的句子，其联合概率越小。因此，在语言模型中，经常使用困惑度来评价。
 - 根据模型计算出的测试数据的概率来评价一个语言模型，或用交叉熵和困惑度等派生测度。
 - 给定一个语言模型，文本T的概率为`p(T)`，在数据T上模型p的交叉熵`H_{p}(T)`定义为：
     ```
     H_{p}(T) = - (1/W_{T}) log_{2}p(T)
     ```
 这里的W_T是以词为单位度量的文本T的长度（可以包括句首标志<BOS>或句尾标志<EOS>）
 - 模型p的困惑度`PP_T(T)`是模型分配给测试集T中每一个词汇的概率的几何平均值的倒数，它和交叉熵的关系为：
     ```
     PP_T(T) = 2^{Hp(T)}
     ```
 显然，交叉熵和困惑度越小越好，这是我们评估一个语言模型的基本准则。
 - 困惑度为每个词条件概率`P(w_{j}^{(i)} | w_{(j−n+1):(j−1)}^{(i)})`的几何平均数的倒数。句子概率越大，困惑度越小，语言模型越好。
 - 假设一个语言模型赋予每个词出现的概率均等，则该语言模型的困惑度为`|V|`

**数据平滑：**
 - 基于统计的语言模型最大似然估计存在一个问题，**数据稀疏问题**，主要是由于训练样本不足而导致密度估计不准确
 - [Zipf定律](https://baike.baidu.com/item/%E9%BD%90%E5%A4%AB%E5%AE%9A%E5%BE%8B/6643264?fr=aladdin)，
 在自然语言的语料库里，一个单词出现的频率与它在频率表里的排名成反比。所以，频率最高的单词出现的频率大约是出现频率第二位的单词的2倍，而出现频率第二位的单词则是出现频率第四位的单词的2倍。
 - 数据稀疏问题最直接的解决方法就是增加训练语料的规模，但是由于大多数自然语言都服从Zipf定律，增加语料库规模的边际效益会随着语料库的增加而递减。
 - 数据稀疏问题的另一种解决方法是平滑技术
 - 在计算条件概率过程中，根据当前的语料S，如果一个词在语料库里不存在，可能会出现概率为0的情况，显然，这个结果不够准确，因为句子`DAVID READ A BOOK`总有出现的可能，其概率应该大于0
 - 因而，必须分配给所有可能出现的字符串一个非零的概率值来避免这种错误的发生
 - 平滑（smoothing）技术就是用来解决这类零概率问题的。术语“平滑”指的是为了产生更准确的概率**来调整最大似然估计**的一种技术，也常称为数据平滑。
 - **“平滑”处理的基本思想是“劫富济贫”，即提高低概率（如零概率），降低高概率，尽量使概率分布趋于均匀**
 - 对于二元语法来说，一种最简单的平滑技术就是假设每个二元语法出现的次数比实际出现的次数多一次，不妨将该处理方法称为**加1法平滑法**，分子加1,分母加`|V|`（当前语料S中，词汇表单词的个数）
 - 数据平滑方法：
    - 加法平滑法
        - 是最简单的，使加1平滑方法通用化
        - 不是假设每一个n元语法发生的次数比实际统计次数多一次，而是假设它比实际出现情况多发生δ次，`0≤δ≤1`
    - 古德-图灵估计法(Good-Turing)
        - Good-Turing估计法是很多平滑技术的核心
        - 其基本思路是：对于任何 一个出现r次的n元语法，都假设它出现了`r*`次
        - Good-Turing方法不能直接用于估计`n_r＝0`的n-gram概率，其中，`n_r`是训练语料中恰好出现r次的n元语法的数目
        - Good-Turing方法不能实现高阶模型与低阶模型的结合，而高低阶模型的结合通常是获得较好的平滑效果所必须的
    - Katz平滑法
        - Katz平滑方法通过加入高阶模型与低阶模型的结合，扩展了Good-Turing估计方法。
        - 所有具有非零计数r的二元语法都根据折扣率`d_r`被减值了
        - Katz平滑方法属于后备（back-off）平滑方法。这种方法的中心思想是，当某一事件在样本中出现的频率大于k时，运用最大似然估计经过 减值来估计其概率。当某一事件的频率小于k时，使用低阶
        的语法模型作为代替高阶语法模型的后备，而这种代替必须受归一化因子α的作 用。对于这种方法的另一种解释是，根据低阶的语法模型分配由于减值而节省下来的剩余概率给未见事件，这比将剩余概率平均分配给未见事件要合理
    - Jelinek-Mercer平滑方法
        - 因为冠词THE要比单词THOU出现的频率高得多。为了利用这种情况， 一种处理办法是在二元语法模型中加入一个一元模型。
        - 一般来讲，使用低阶的n元模型向高阶n元模型插值是有效的，因为当没有足够的语料估计高阶模型的概率时，低阶模型往往可以提供有用 的信息。
        - 第n阶平滑模型可以递归地定义为n阶最大似然估 计模型和n-1阶平滑模型之间的线性插值
    - Witten-Bell平滑方法
        - 它可以认为是Jelinek-Mercer平滑算法的一个实例
        - n阶平滑模型被递归地定义为n阶最大似然模型和n-1阶平滑模型的线性插值
    - 绝对减值法
        - 绝对减值法（absolute discounting）类似于Jelinek-Mercer平滑算法，涉及高阶和低阶模型的插值问题
        - 通过从每个非零计数中减去一个固定值`D≤1`的方法来建立高阶分布
    - Kneser-Ney平滑方法
    - Church-Gale平滑方法
    - 贝叶斯平滑方法
    - 修正的Kneser-Ney平滑方法
 - 平滑方法的比较：
    - **不管训练语料规模多大，对于二元语法和三元语法而言，Kneser-Ney平滑方法和修正的Kneser-Ney平滑方法的效果都好于其他所有的平滑方法**。
    - 一般情况下，Katz平滑方法和Jelinek-Mercer平滑方法也有较好的表现，但与上述两者平滑方法相比稍有逊色。
    - 在稀疏数据的情况下，Jelinek-Mercer平滑方法优于Katz平滑方法，而在有大量数据的情况下，Katz平滑方法则优于Jelinek-Mercer平滑方法
    - 插值的绝对减值平滑方法和后备的Witten-Bell平滑方法的表现最差
    - 除了对于很小的数据集以外，插值的绝对减值平滑方法一般优于留存插值方法的Jelinek-Mercer平滑方法
    - 而Witten-Bell平滑方法则表现较差，对于较小的数据集，该方法比留存插值方法的Jelinek-Mercer平滑方法差得多
    - 对于大规模数据集而言，这Witten-Bell平滑方法和插值的绝对减值平滑方法都比留存插值方法的Jelinek-Mercer平滑方法优越得多，甚至可以与Katz平滑方法和Jelinek-Mercer平滑方法相匹敌
    - 平滑方法的相对性能与训练语料的规模、n元语法模型的阶数和训练语料本身有较大的关系，其效果可能会随着这些因素的不同而出现很大的变化
    - 下列因素对于平滑算法的性能有一定的影响：
        - 影响最大的因素是采用修正的后备分布
        - 绝对减值优于线性减值
        - 从性能上来看，对于较低的非零计数，插值模型大大地优于后备模型
        - 增加算法的自由参数，并在留存数据上优化这些参数，可以改进算法的性能

**语言模型的自适应方法：**
 - 经常遇到的问题：
    - **模型对跨领域的脆弱性：**在训练语言模型时所采用的语料往往来自多种不同的领域，这些综合性语料难以反映不同领域之间在语言使用规律上的差异，而**语言模型恰恰对于训练文本的类型、主题和风格等都十分敏感**
    - **独立性假设的无效性：**n元语言模型的独立性假设前提是一个文本中的当前词出现的概率只与它前面相邻的n-1个词相关，但**这种假设在很多情况下是明显不成立的**。
 - 香农实验表明，相对而言，人更容易运用特定领域的语言知识、常识和领域知识进行推理以提高语言模型的性能（预测文本的下一个成分）。因此，**为了提高语言模型对语料的领域、主题、类型等因素的适应性，提出了自适应语言模型**
 - 语言模型自适应方法：
    - 基于缓存的语言模型
        - 在文本中刚刚出现过的一些词在后边的句子中再次出现的可能性往往较大，比标准的n元语法模型预测的概率要大
        - 针对这种现象，cache-based自适应方法的基本思路是，语言模型通过n元语法的线性插值求得，插值系数λ可以通过EM算法求得
        - （正常的基于缓存的语言模型）常用的方法是，在缓存中保留前面的K个单词，每个词wi的概率（缓存概率）用其在缓存中出现的相对频率计算得出
        - （衰减的基于缓存的语言模型）缓存中每个词对当前词的影响随着与该词距离的增大呈指数级衰减，越是临近的词，对缓存概率的贡献越大
        - cache-based自适应方法减低了语言模型的困惑度，衰减的比正常的给予缓存的语言模型对降低语言模型的困惑度效果更好
        - 黄非等（1999）提出了利用特定领域中少量自适应语料，在原词表中通过**分离通用领域词汇和特定领域词汇，并自动检测词典外领域关键词实现词典自适应**，然后结合基于缓存的方法实现语言模型的自适应方法
        - 曲卫民等（2003）通过采用TF-IDF公式代替原有的简单频率统计法，**建立基于记忆的扩展二元模型，并采用权重过滤法以节省模型计算量**，实现了对基于缓存记忆的语言模型自适应方法的改进
        - 张俊林等 （2005）也对基于记忆的语言模型进行了扩展，利用汉语义类词典，**将与缓存中所保留词汇语义上相近或者相关的词汇也引入缓存**，在一定程度上提高了原有模型的性能
    - 基于混合方法的语言模型
        - 基于混合方法的自适应语言模型针对的问题是：
            - 由于大规模训练语料本身是异源的，来自不同领域的语料无论在主题方面，还是在风格方面，或者同时在这两方面都有一定的差异
            - 而测试语料一般是同源的，因此，为了获得最佳性能，语言模型必须适应各种不同类型的语料对其性能的影响。
        - 基本思想是：
            - 将语言模型划分成n个子模型M1，M2，…，Mn，整个语言模型的概率通过下面的线性插值公式计算得到
                ```
                p(w_i|w_{1}^{i-1}) = sum_{j=1}^n lambda_j p_{M_j}(w_i|w_{1}^{i-1})
                ```
            lambda属于0到1，之和为1，可以通过EM算法计算
        - 基于这种思想，该适应方法针对测试语料的实现过程包括下列步骤：（即有针对性的训练子语言模型）
            - 对训练语料按来源、主题或类型等（不妨按主题）聚类；
            - 在模型运行时识别测试语料的主题或主题的集合；
            - 确定适当的训练语料子集，并利用这些语料建立特定的语言模型；
            - 利用针对各个语料子集的特定语言模型和线性插值公式，获得整个语言模型。
    - 基于最大熵的语言模型
        - 上面两种思路都是分别建立各个子模型，然后将子模型的输出组合起来
        - 基于最大熵的语言模型采用不同的思路，**即通过结合不同信息源的信息构建一个语言模型。每个信息源提供一组关于模型参数的约束条件，在所有满足约束的模型中，选择熵最大的模型**
        - 例子：
            ```
            考虑两个语言模型M1和M2，假设M1是标准的二元模型，表示为f函数：
            p_{M_1}(w_i|w_{1}^{i-1}) = f(w_i, w_{i-1})
            
            M2是距离为2的二元模型（distance-2 bigram），定义为g函数
            p_{M_2}(w_i|w_{1}^{i-1}) = g(w_i, w_{i-2})
            
            可以用线性插值方法取这两个概率的平均值，用后备方法选择其中一个进行数据平滑。
            
            为了考虑**远距离的文本历史信息**，以弥补一般语言模型仅仅利用近距离历史的不足，采用**触发器对**作为信息承载成分的思想。
             - 如果一个词序列A与另一个词序列B密切相关，那么，A→B被看作一个触发器对，
             - 其中，A为触发器，B为被触发的序列。
             - 当A出现在一个文本中时，它触发B，从而引起B的概率估计发生变化。
             - 如果把B看作当前词，A为历史h中的某个特征，那么，可以将一个二值的触发器对A→B形式化为一种约束函数f_{A→B}
            ```
        - 由于最大熵模型能够较好地将来自不同信息源的模型结合起来，获得性能较好的语言模型,因此，有些学者研究将基于主题的语言模型 （topic-based LM）（主题条件约束）与n元语法模型
        相结合，用于对话语音识别、信息检索IR和隐含语义分析(LSA)
 - 语言模型的自适应方法是改进和提高语言模型性能的重要手段之一。其性能表现与语料本身的状况（领域、主题、风格等）以及选用的统计基元等密切相关


## 神经网络语言模型
**一般概念：**
 - 在统计语言模型中，一个关键的问题是估计`P(Wt|W_{1:(t−1)})`，即在时刻（或位置）t，给定历史信息`h_t = w_{1:(t−1)}`条件下，词汇表V中的每个词v_k出现的概率。
 这个问题可以转换为一个类别数为|V|的多类分类问题，估计的词汇表中第k个词出现的后验概率。
 - 这样，我们就可以**使用机器学习中的不同分类器来估计语言模型的条件概率**。这里我们关注基于神经网络模型的方法，这一类方法可以统称为神经网络语言模型（NNLM）
 
**神经网络语言模型:**
 - 神经网络语言模型可以分为三个部分：
    - 输入层
        - 将语言符号序列`w_{1:(t-1)}`输入到神经网络模型中（以词嵌入矩阵的形式，稠密实数值向量）
        - 词嵌入矩阵`M ∈ R^{d1×|V|}`中，第k列向量`k ∈ R^{d1}`表示词汇表中第k个词对应的稠密向量
        - 通过直接映射，得到历史信息`w_{1:(t-1)}`每个词对应的向量表示`v_{w1},···,v_{w_{t−1}}`
    - 隐藏层
        - 隐藏层可以是不同类型的网络，前馈神经网络和循环神经网络，其输入为词向量`v_{w1},···,v_{w_{t−1}}`，输出为一个可以表示历史信息的向量`h_t`
        - 常见的网络类型：
            - 简单平均
                ```
                h_t = sum_{1}^{t-1} C_i v_{w_i}
                
                其中，C_i为每个词的权重。权重可以和位置相关，也可以和位置无关。位置无关的权重可以设置为C_i = 1/(t−1)
                ```
            - 前馈神经网络
                - 前馈神经网络要求输入的大小是固定的。
                - 因此，和n元模型类似，假设历史信息只包含前面n−1个词。首先将这n−1个的词向量拼接为一个维度为`d1 × (n−1)`的向量`x_t`
                - 然后将`x_t`输入到由多层前馈神经网络构成的隐藏层，最后一层隐藏层的输出`h_t`
                - 前馈网络的结构可以任意设置：
                    - 比如，只含一层隐藏层的模型为`h_t = tanh(W x_t + c)`
                    - 也可以包含跳层连接`h_t = x_t ⊕ tanh(W x_t + c)`
            - 循环神经网络
                - 和前馈神经网络不同，循环神经网络可以接受变长的输入序列，依次接受输入`v_{w1},···,v_{w_{t−1}}`，得到t时刻的隐藏状态`h_t`
        - 前馈网络语言模型和循环网络语言模型的不同之处在于**循环神经网络利用隐藏状态来记录以前所有时刻的信息，而前馈神经网络只能接受前(n-1)个时刻的信息。
    - 输出层
        - 输出层为大小为`|V|`，其接受的输入为历史信息的向量表示`h_t ∈ R^{d2}`，输出为**词汇表中每个词的后验概率**。
        - 在神经网络语言模型中，一般使用`softmax`分类器
            ```
            y_t = softmax(O h_t + b)
            
            输出向量y_t ∈ R^{|V|}为一个概率分布，
            O ∈ R^{|V|×d2}是最后一层隐藏层到输出层直接的权重。
            O也叫做**输出词嵌入矩阵**，矩阵中每一行也可以看作是一个词向量。
            ```
 - 大词汇表上`softmax`计算的改进 (加速语言模型训练速度)
    - 而归一化时要计算配分函数，即对词汇表中所有的词进行计算并求和，计算开销非常大，在实践中，经常采样一些近似估计的方法来加快训练速度。主要的加快训练速度的方法有两类：
    - 一类是**层次化的`softmax`计算**，将标准`softmax`函数的扁平结构转换为层次化结构
        - 先来考虑两层的来组织词汇表，即将词汇表中词分成k组，每一个词只能属于一个分组，每组大小为`|V|/k`。假设词w所属的组为`c(w)`，则
            ```
            P(w|h) = P(w,c(w)|h)
                   = P(w|c(w),h) × P(c(w)|h)
            
            其中，P(c(w)|h)是给定历史信息h条件下，类c(w)的概率，
            P(w|c(w), h)是给定历史信息h和类c(w)条件下，词w的概率
            一个词的概率可以分解为两个概率的乘积，而它们都是利用神经网络来估计，
            计算softmax函数时分别只需要做|V|/k和k次求和，从而就大大提高了softmax函数的计算速度
            ```
        - 一般对于词汇表大小|V|，我们将词平均分到`sqrt(|V|)`个分组中，每组`sqrt(|V|)`个词，这样通过一层的分组，我们可以将`softmax`计算加速`1/2 × sqrt(|V|)`倍。
        - 为了进一步降低`softmax`的计算复杂度，我们可以更深层的树结构来组织词汇表。假设用二叉树来组织词汇表中的所有词，二叉树的叶子节点代表词汇表中的词，非叶子节点表示不同层次上的类别。
        如果我们将二叉树上所有左链接标记为0，右链接标记为1。每一个词可以用根节点到它所在的叶子之间路径上的标记来进行编码。
        - 词v的条件概率为：
            ```
            P(v|h) = P(b(v,j)|n(v,j-1), h)
            
            n(v,j-1)是一个非叶子结点，
            因为b(v, j) ∈ {0, 1}，因此可以看作是二分类问题，可以使用logistic回归来进行计算
            若使用平衡二叉树来进行分组，则条件概率估计可以转换为log_{2}|V|个二分类问题，
            这时softmax函数可以用logistic函数代替,softmax的计算可以加速 |V|/(log_{2}|V|)倍
            ```
        - 将词汇表中的词按照树结构进行组织，有以下方式：
            - 利用人工整理的词汇层次结构，比如利用WordNetMiller系统中的`IS-A`关系（is a, 即上下位关系）。例如，“狗”是“动物”的下位词。因为WordNet的层次化结构不是二叉树，因此需要通过进一步聚类来转换为二叉树。
            - 使用Huffman编码。**Huffman编码对出现概率高的词使用较短的编码，出现概率低的词则使用较长的编码。因此训练速度会更快。**
    - 另一类是**基于采样的方法**，通过采样来近似计算更新梯度
        - 神经网络语言模型的目标函数，如果用随机梯度下降来求解参数θ，在计算每个样本的更新梯度时需要**两次计算在整个词汇表上的求和**，由于语言模型中的词汇表都比较大，训练速度非常慢
        - 为提高训练效率，可以用采样方法来近似地估计求导后公式中的期望，两种在语言模型中比较有效的采样方法：
            - 重要性采样
                - 用一个容易采样的参考分布Q，来近似估计分布P
                - 经过对公式的变换，将分布P上的期望转换为分布Q上的期望
                - 分布Q应与分布P尽可能接近，且从Q采样的代价要比较小
                - 在语言模型中，分布Q可以采用n元语言模型的分布函数
                - 有了分布Q之后，可以从中独立抽取k个样本来近似求解公式
                - 通过Q的采样来估计目标分布P的方法就叫做重要性采样
                - 重要性采样是一种非均匀采样
                - 最后，可以得到每个样本目标函数关于θ的梯度的近似值
                - 重要性采样相当于采样了一个词汇表的子集，然后在这个子集上求梯度的期望
                - 采样的数量n越大，近似越接近正确值，在实际应用中，n取100左右就能够以足够高的精度对期望做出估计
                - 通过重要性采样的方法，训练速度可以加速`|V|/k`倍，k为子词汇表的大小
                - 但是，其效果依赖于建议分布`Q(w′|h_t)`的选取，如果选取不合适，会造成梯度估计非常不稳定，经常使用一元模型的分布函数
            - 噪声对比估计
                - 基本思想是将密度估计问题转换为二分类问题，从而降低计算复杂度
                - 比如我们教小孩认识“苹果”，往往会让小孩从一堆各式各样的水果中找出哪个是“苹果”。通过不断的对比和纠错，最终小孩会知道了解“苹果”的特征，并很容易识别出“苹果”
                - 噪声对比估计的数学描述如下：
                    - 假设有三个分布，一个是需要建模真实数据分布`P(x)`
                    - 第二是模型分布`P_{θ}(x)`，我们期望调整模型参数为θ来使得`P_{θ}(x)`来拟合真实数据分布
                    - 第三个是噪声分布`Q(x)`，用来对比学习。
                    - 从`P(x)`和`Q(x)`的混合分布中抽取一个样本x，从`P(x)`中抽取的样本叫做“真实”样本，从`Q(x)`中抽取的样本叫做噪声样本。
                    - 需要建立一个“辨别者”D来判断样 本x是真实样本还是噪声样本。
                    - 噪声对比估计是通过调整模型`P_{θ}(x)`使得“判别者”D很容易能分别出样本x来自哪个分布。
                - 一般噪声样本的数量要比真实样本大很多。这里假设噪声样本的数量是真实样本的k倍
                - 噪声对比估计的目标是将模型分布`P_{θ}(x)`和噪声分布`Q(x)`区别开来，可以看作是二分类问题。通过不断比较真实样本和噪声样本，来学习模型参数θ。
                - 噪声对比估计相当于用判别式的准则`L_θ`来训练一个生成式模型`P_{θ}(x)`，其思想与生成式对抗网络类似。不同之处在于，在噪声对比估计中的“判别者”D是通过Bayes
                公式计算得到，而生成式对抗网络的“判别者”D是一个需要学习的神经网络。
                - 噪声对比估计是找到一个模型`P_{θ}(x)`使得“判别者”D很容易能分别出样本x来自哪个分布。
                - 噪声对比估计方法的一个特点是会促使未归一化分布`exp(s(w, h; θ))`可以自己学习到一个近似归一化的分布，并接近真实的数据分布`P(w|h)`
                - 在噪声对比估计中，噪声分布`Q(w)`的选取也十分关键
    - 总结：
        - 基于采样的方法并不改变模型的结构，只是近似计算更新梯度。因此这类方法虽然在训练时可以显著提高模型的训练速度，但是在测试阶段依然需要计算配分函数。
        - 而基于层次化`softmax`的方法改变的模型的结构，在训练和测试时都可以加快计算速度。

## 基于分布式假设的词嵌入学习
**一般概念：**
 - 通过神经网络语言模型，我们可以在大规模的无标注语料上进行训练，来得到一组好的词向量。
 - 这些词向量可以作为预训练的参数，再代入到特定任务中进行精调。
 - 使用NNLM来训练词嵌入有两个不足：
    - 一是即使使用改进的NNLM，其训练也需要大量的计算资源训练，训练时间非常长。
    - 二是NNLM的优化目标是降低语言模型的困惑度，和词嵌入的好坏并不是强相关关系。
 - 虽然训练一个好的语言模型会得到一组好的词嵌入，但是一组好的词嵌入却不一定要使得语言模型的困惑度降低。

下边几种是**不通过优化语言模型而直接学习词嵌入**的方法：

**连续词袋模型和Skip-Gram模型：**
 - 词嵌入学习工具word2vec中包含的两种模型：
    - 连续词袋模型和Skip-Gram模型
    - 这两种模型虽然依然是基于语言模型，但**训练目标是得到一组较好的词嵌入而不是降低语言模型的困惑度**。
    - 为了提高训练效率，这两种模型都通过简化模型结构大幅降低复杂度，并提出两种高效的训练方法（**负采样**和**层次化softmax**）来加速训练
    - 在标准的语言模型中，当前词`w_t`依赖于其前面的词`w_{1:(t−1)}`。而在连续词袋模型和Skip-Gram模型中，当前词`w_t`**依赖于其前后的词**。
 - 模型结构：
    - 连续词袋模型（CBOW）
        ```
        给定一个词w_t的其上下文：
        c_t = w_{t-n},···,w_{t-1}, w_{t+1},···,w_{t+n}
        
        和标准语言模型不同，上下文c_t可以同时取左右两边的n个词。
        
        连续词袋模型CBOW是该词w_t出现的条件概率为:
        P(w_t|c_t) = softmax(v'_{w_t}^T c_t)
                   = exp(v'_{w_t}^T c_t) / ( sum_{w'∈V} (exp(v'_{w_t}^T c_t)) )
        
        其中，c_t表示上下文信息。
        ```
        - 在连续词袋模型中，就**直接把隐藏层去掉,大大减少了计算量**,提高了计算速度,然后用更多的数据来训练模型,最后的效果也不错
        - 给定一个训练文本训练`w1,···,wT`，连续词袋模型的目标函数为：
            ```
            L_θ = - 1/T sum_{t=1}^T log p(w_t|c_t)
            ```
        - 即通过当前词的两边词（上下文信息）来计算当前词的后验概率，没有隐藏层，直接进行计算
    - Skip-Gram模型
        - Skip-Gram模型给定一个词`w_t`，**预测词汇表中每个词出现在其上下文中的概率**。
            ```
            P(w_{t+j}|w_t) = softmax( v_{w_t}^T  v'_{w_{t+j}} )
                           = exp( v_{w_t}^T  v'_{w_{t+j}} ) / ( sum_{w'∈V} exp( v_{w_t}^T  v'_w' ) )
            
            其中，v_w表示词w在输入词嵌入矩阵中的词向量，
            v'_w表示词w在输出词嵌入矩阵中的词向量。
            ```
        - Skip-Gram模型没有隐藏层，`h_t`直接等于词嵌入`v_{w_t}`。
        - 给定一个训练文本训练`w1,···,wT`，Skip-Gram模型的目标函数为：
            ```
            L_θ = - 1/T sum_{t=1}^T sum_{-n<=j<=n,j!=0} log P(w_{t+j}|w_t)
            ```
 - 训练方法：
    - 在Word2Vec中，连续词袋模型和Skip-Gram模型都可以通过两种训练方法（层次化`softmax`和负采样）来加速训练。
        - 层次化`softmax`，。在Word2Vec中采样了Huffman树来进行词汇表的层次化
        - 负采样：该方法可以看成是噪声对比估计方法的一个简化版本。
            ```
            给定上下文信息c，对于词汇表中每一个词w，
            w来自于真实分布的概率为：
            P(y=1|w,c) = exp(s(w, c; θ)) / ( exp(s(w, c; θ)) + 1 )
                       = 1 / ( 1 + exp(−s(w, c; θ)) )
                       = σ(s(w, c; θ))
            
            其中，σ为logistic函数，s(w, c; θ)为模型得分
            σ = 1 / (1+exp(−x))
            
            给定一个训练文本序列w1,···,wT，在位置t时，连续词袋模型和Skip-Gram模型的s(w_t, c_t; θ)定义如下：
                - 在连续词袋模型中，预测目标w为当前词w_t，   c为上下文词
                    s(wt, ct; θ) = v'_{w_t}^T sum_{-n<=j<=n,j!=0} v_{w_{t+j}}
                - 在Skip-Gram模型中，预测目标w为上下文词，  c为当前词w_t
                    s(wt, ct; θ) = v_{w_t}^T v'_{w_{t+j}} , −n ≤ j ≤ n, j ̸= 0
            ```
            - 使用负采样方法进行训练时，对于每个正例(w_t, c_t)，用噪声分布Q(w)中随机采样k个负例。k的取值由数据大小决定，
            通常小规模数据k的取值范围在5∼20，而大规模数据k可以非常小2∼5
            - 和噪声对比估计类似，负采样方法的目标函数也是一个二分类问题
            - 通过logistic回归来区分目标词w是来自真实分布还是噪声分布`Q(w)`
            - 和噪声对比估计不同的是，噪声分布`Q(w)`只是用来采样，而不参与计算。因此，噪声对比估计可以近似语言模型，而负采样不可以。
    - Word2Vec加速技巧：
        - **删除隐藏层**，得到上下文c的表示后，直接输入到`softmax`分类器来预测输出。
        也就是说，整个网络的参数只有两个词嵌入表：输入词嵌入表和输出词嵌入表；
        - 使用**层次化`softmax`或负采样**进行加速训练
        - **去除低频词**。出现次数小于一个预设值minCount的词直接去除。
        - **对高频词进行降采样**：
            ```
            根据下面公式算出的概率P_{discard}(w_t)来跳过词w_t
            这样可以节省时间而且可以提高非高频词的准确度。
            
            P_{discard}(w_t) = 1 - sqrt( m / U(w_t) )
            
            其中，m为设定好的阈值，一般取10−5，U(w_t)为w_t的一元语言模型频率。
            也就是说，对于词，U(w_t)越大（高频词的较大），P_{discard}(w_t)就越大，用来以该概率来跳过高频词
            ```
        - **动态上下文窗口大小**。指定一个最大窗口大小值N，对于每个词，从\[1,N]中随机选取一个值n来作为本次的上下文窗口大小，
        从当前词左右各选取n个词。这样上下文更侧重于邻近词。
        - **噪声分布使用一元语言模型**`U(x)`的`U(x)^{3/4} / Z`，Z为归一化因子。**相当于对高频词进行降采样，对低频词进行上采样**。
 - 从连续词袋模型和Skip-Gram模型的定义可以看出，**对于上下文相似的词，其向量也会相似**。这和分布式假设的定义十分吻合。
 **分布式假设的定义为如果两个词的上下文分布相似，那么这两个词的词义也是相似的**。

**总结：**
 - 词嵌入，即词的分布式表示，是NN来NLP的前提和关键因素。
 - NNLM需要给词汇表中的每一个词都赋予一个概率，即一个类别数为|V|的多类分类问题，类别数远大于一般的机器学习任务。因此在进行`softmax`归一化时计算代价很高。
    - Bengio \[2008]提出了利用重要性采样来加速`softmax`的计算
    - Mnih \[2013]提出了噪声对比估计来计算非归一化的条件概率。
    - Morin and Bengio \[2005]最早使用了层次化`softmax`函数来近似扁平的`softmax`函数。
 - 但是通过NNLM来预训练词嵌入由两个不足：
    - 一是即使使用改进的NNLM，其训练也需要大量的计算资源训练，训练时间非常长。
    - 二是NNLM的优化目标是降低语言模型的困惑度，和词嵌入的好坏并不是强相关关系。
 - Mikolov \[2013a]提出了两种非常简化的模型：连续词袋模型和Skip-Gram模型。其核心思想是当语料足够大时，简单的模型也能得到较好的词表示。
 - 大量的词是有多种义项，在不同上下文中，词的意义也不同，如果只赋予每一个词一个词向量或多个词向量，会出现问题。
 可参考基于上下文的词嵌入模型\[Huang et al., 2012, Neelakantan et al., 2014]。


**GloVe：**



## word2vec数学原理
是一个开源工具包，用来获取词的向量


## Beam Search算法


## 序列生成算法


## 一些问题
**线性插值法和Attention机制：**


