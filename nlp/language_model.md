# 语言模型

> 目前主要采用的是n元语法模型，这种模型构建简单、直接，但同时也因为数据缺乏 而必须采取平滑（smoothing）算法

**参考文献：**
 - 《统计自然语言处理》 by 宗成庆
 - 《深度学习》(花书) by Bengio
 - 《神经网络与深度学习》 by 邱锡鹏


**一般概念：**
 - 词的局部表示就是one-hot向量表示，即词袋模型(Bag-of-Words, BOW)，即将文本看成是词的集合，不考虑词序信息。又叫做向量空间模型(Vector Space Model, VSM)
 - 将高维的局部表示向量空间`R^{|V|}`映射到一个非常低维的空间`R^d`，V为词表，。这个低维空间中的表示就是**分布式表示**
 - 对于词的分布式表示（即低维稠密向量表示），我们经常叫做词嵌入（Word Embeddings）
 - 在机器学习中，**嵌入**通常指将一个度量空间X中的对象映射到另一个低维的度量空间Y中，并进行可能保持不同对象之间拓扑关系。
 - 在one-hot向量空间中，每个词都位于坐标轴上，每个坐标轴上一个词；而在低维的嵌入空间中，每个词都不在坐标轴上，词之间可以计算相似度。

## 统计语言模型
**定义：**
 - 统计语言模型把语言（词/字/字符的序列）看作一个随机事件，并赋予相应的概率来描述其属于某种语言集合的可能性。
 - 给定一个词汇集合V，对于一个由V中的词构成的序列`S=⟨w1,···,wT⟩ ∈ V^n`，统计语言模型赋予这个序列一个概率`P(S)`，来衡量S符合自然语言的语法和语义规则的置信度。

简单地说，统计语言模型就是用来**计算一个句子的概率的模型**。统计语言模型可以估计出自然语言中每个句子出现的可能性，而不是简单的判断该句子是否符合文法。

**一些性质：**
 - 语言模型的两个基本功能是：
    - 判断一段文本是否符合一种语言的语法和语义规则
    - 生成符合一种语言语法或语义规则的文本
 - [多项分布](https://baike.baidu.com/item/%E5%A4%9A%E9%A1%B9%E5%88%86%E5%B8%83/6548502?fr=aladdin)。
 多项式分布是二项式分布的推广。二项分布的典型例子是扔硬币，硬币正面朝上概率为p,重复扔n次硬币，k次为正面的概率即为一个二项分布概率。把二项分布公式推广至多种状态，就得到了多项分布。
 - 在传统的统计语言模型中，一般**假设语言是服从多项分布**，并**利用最大似然估计来求解多项分布的参数**(也就是求解前n-1个词的条件下，第n个词的概率)。
 - **最大似然估计等价于频率估计**


**N元语法：**
 - 一个语言模型**通常构建为字符串s的概率分布`p(s)`，试图反应的是字符串s作为一个句子出现的频率。**
 - 与语言学中不同，语言模型与句子是否合乎语法是没有关系的，即使一个句子完全合乎语法逻辑，我们仍然可以认为它出现的概 率接近为零
 - 句子`s=w_{1}w_{2}...w_{l}`，其概率计算公式可以表示为：
     ```
     p(s) = p(w_1)p(w_2|w_1)p(w_3|w_1w_2)...p(w_l|w_1...w_{l-1})
          = \Pi_{i=1}^l p(w_i|w_1...w_{i-1})
     ```
 - n元语法（或n元文法，n-gram），用来解决当前词的历史词过多时计算上述公式参数过多的问题
    - 产生第i个词的概率是由已经产生的i-1个词决定的，把前边的i-1个词成为第i个词的历史
    - 随着历史长度的增加，不同的历史数目按指数级增长，这使我们几乎不可能从训练数据中正确地估计出这些参数，实际上，绝大多数历史根本就不可能在训练数据中出现。
    - 为了解决这个问题，可以将历史w1w2…wi-1按照某个法则映射到等价类E(w1w2…wi-1)，而等价类的数目远远小于不同历史的数目。
    - 如果假定`p(w_i|w_1,w_2,...,w_{i-1}) = p(w_i|E)`，那么，自由参数的数目就会大大地减少
    - 一种比较实际的做法是，将两个历史映射到同一个等价类，当且仅当这两个历史**最近的`n-1`个词相同**
    - 满足上述条件的语言模型称为n元语法
    - 通常情况下，n的取值不能太大，否则，等价类太多，自由参数过多的问题仍然存在。
        - 在实际应用中，取n=3的情况较多（即当前词仅与它之前的前两个词相关，3-gram）。
        - 当n=1时，即出现在第i位上的词w_i独立于历史时（不与之前的任何词相关），一元文法被记作unigram，或uni-gram，或monogram
        - 当n=2时，即出现在第i位上的词w_i仅与它前面的一个历史词w_{i-1}有关，**二元文法模型被称为一阶马尔可夫链**(Markov chain)，记作bigram或bi-gram
        - 当n=3时，即出现在第i位上的词w_i仅与它前面的两个历史词w_{i-2}w_{i-1}有关，**三元文法模型被称为二阶马尔可夫链**，记作trigram或tri-gram
    - 以二元语法模型为例，根据前面的解释，我们可以近似地认为，一个词的概率只依赖于它前面的一个词，那么概率计算公式近似为：
        ```
        p(s) = \Pi_{i=1}^l p(w_i|w_1...w_{i-1})
             = \Pi_{i=1}^l p(w_i|w_{i-1})
        ```
        - 为了让上式对于i=1有意义，我们在句子开头加上一个句首标记<BOS>，即假设w_0就是<BOS>
        - 为了使得所有的字符串的概率之和`sum_{s}p(s)`等于1，需要在句子结尾再放一个句尾标记<EOS>，并且使之包含在上边等式的乘积中，
        (如果不做这样的处理，所有给定长度的字符串的概率和为1，而所有字符串的概率和为无穷大，因为不知道什么时候是句子的结束，当知道句子的长度时，在知道在哪结束句子)
        - 为了估计条件概率`p(w_i|w_{i-1})`，可以简单地计算二元语法`w_{i-1}w_i`在某一文本中**出现的频率**，然后归一化。
        如果用`c(w_{i-1}w_i)`表示二元语法`w_{i-1}w_i`**在给定文本中的出现次数**，我们可以采用下面的计算公式:
            ```
            p(w_i|w_{i-1}) = c(w_{i-1}w_i) / sum_{w_i}c(w_{i-1}w_i)
            ```
        分母`sum_{w_i}c(w_{i-1}w_i)`表示计算历史`c(w_{i-n+1}^{i-1})`的数目，具体见下边例子
        - 用于构建语言模型的文本称为训练语料。对于n元语法模型，使用的训练语料的规模一般要有几百万个词。上式用于估计`p(w_i|w_{i-1})`的方法称为`p(w_i|w_{i-1})`的**最大似然估计**
        - 计算例子：
            ```
            假设语料S由下边的3个句子构成：
            ("BROWN READ HOLY BIBLE", 
             "MARK READ A TEXT BOOK", 
             "HE READ A BOOK BY DAVID")
             
            用计算最大似然估计的方法计算概率 p(BROWN READ A BOOK)：
            p(BROWN|<BOS>) = c(<BOS> BROWN) / sum_{w}c(<BOS> w) = 1/3
            p(READ|BROWN) = c(BROWN READ) / sum_{w}c(BROWN w) = 1/1
            p(A|READ) = c(READ A) / sum_{w}c(READ w) = 2/3
            p(BOOK|A) = c(A BOOK) / sum_{w}c(A w) = 1/2
            p(<EOS>|BOOK) = c(BOOK <EOS>) / sum_{w}c(BOOK w) = 1/2
            
            因此：
            p(BROWN READ A BOOK) = p(BROWN|<BOS>) ×
                                   p(READ|BROWN) ×
                                   p(A|READ) ×
                                   p(BOOK|A) ×
                                   p(<EOS>|BOOK)
                                 = 1/3 × 1/1 × 2/3 × 1/2 × 1/2
                                 = 0.06
            ```

**n-gram与马尔可夫链的关系：**
 - 二元文法模型被称为一阶马尔可夫链，三元文法模型被称为二阶马尔可夫链

**困惑度：**
 - 困惑度(Perplexity)可以用来衡量一个分布的不确定性，我们也可以用困惑度来衡量两个分布之间差异
 - 对于一个未知的数据分布`P_{data}(X)`，和一个模型分布`P_{model}(X)`，我们从`P_{data}(X)`中采样出一组测试样本`x1,···,xN`，模型分布`P_{model}(X)`的困惑度为：
     ```
     2^{ H(P'_{data}, P_{model}) } = 2^{-1/N sum_{i=1}^N log_{2}P_{model}(x_i)}
     
     其中H(P'_{data}, P_{model})为样本的经验分布P'_{data}和模型分布P_{model}之间的交叉熵，  
     也是所有样本上的负对数似然函数
     ```
 - 模型的困惑度**可以衡量模型估计分布与样本经验分布之间的契合程度**，困惑度越低则两个分布越接近。因此，模型`P_{model}`的好坏可以用困惑度来评价。

**语言模型性能评价：**
 - 一个好的语言模型应该使得测试集合中的**句子的联合概率尽可能高**
 - 语言模型最直接的评价就是测试集中所有句子的联合概率，一个好的语言模型在测试集上的概率越高越好。但是联合概率的缺点是一般都比较小，并且和句子长度有关。越长的句子，其联合概率越小。因此，在语言模型中，经常使用困惑度来评价。
 - 根据模型计算出的测试数据的概率来评价一个语言模型，或用交叉熵和困惑度等派生测度。
 - 给定一个语言模型，文本T的概率为`p(T)`，在数据T上模型p的交叉熵`H_{p}(T)`定义为：
     ```
     H_{p}(T) = - (1/W_{T}) log_{2}p(T)
     ```
 这里的W_T是以词为单位度量的文本T的长度（可以包括句首标志<BOS>或句尾标志<EOS>）
 - 模型p的困惑度`PP_T(T)`是模型分配给测试集T中每一个词汇的概率的几何平均值的倒数，它和交叉熵的关系为：
     ```
     PP_T(T) = 2^{Hp(T)}
     ```
 显然，交叉熵和困惑度越小越好，这是我们评估一个语言模型的基本准则。
 - 困惑度为每个词条件概率`P(w_{j}^{(i)} | w_{(j−n+1):(j−1)}^{(i)})`的几何平均数的倒数。句子概率越大，困惑度越小，语言模型越好。
 - 假设一个语言模型赋予每个词出现的概率均等，则该语言模型的困惑度为`|V|`

**数据平滑：**
 - 基于统计的语言模型最大似然估计存在一个问题，**数据稀疏问题**，主要是由于训练样本不足而导致密度估计不准确
 - [Zipf定律](https://baike.baidu.com/item/%E9%BD%90%E5%A4%AB%E5%AE%9A%E5%BE%8B/6643264?fr=aladdin)，
 在自然语言的语料库里，一个单词出现的频率与它在频率表里的排名成反比。所以，频率最高的单词出现的频率大约是出现频率第二位的单词的2倍，而出现频率第二位的单词则是出现频率第四位的单词的2倍。
 - 数据稀疏问题最直接的解决方法就是增加训练语料的规模，但是由于大多数自然语言都服从Zipf定律，增加语料库规模的边际效益会随着语料库的增加而递减。
 - 数据稀疏问题的另一种解决方法是平滑技术
 - 在计算条件概率过程中，根据当前的语料S，如果一个词在语料库里不存在，可能会出现概率为0的情况，显然，这个结果不够准确，因为句子`DAVID READ A BOOK`总有出现的可能，其概率应该大于0
 - 因而，必须分配给所有可能出现的字符串一个非零的概率值来避免这种错误的发生
 - 平滑（smoothing）技术就是用来解决这类零概率问题的。术语“平滑”指的是为了产生更准确的概率**来调整最大似然估计**的一种技术，也常称为数据平滑。
 - **“平滑”处理的基本思想是“劫富济贫”，即提高低概率（如零概率），降低高概率，尽量使概率分布趋于均匀**
 - 对于二元语法来说，一种最简单的平滑技术就是假设每个二元语法出现的次数比实际出现的次数多一次，不妨将该处理方法称为**加1法平滑法**，分子加1,分母加`|V|`（当前语料S中，词汇表单词的个数）
 - 数据平滑方法：
    - 加法平滑法
        - 是最简单的，使加1平滑方法通用化
        - 不是假设每一个n元语法发生的次数比实际统计次数多一次，而是假设它比实际出现情况多发生δ次，`0≤δ≤1`
    - 古德-图灵估计法(Good-Turing)
        - Good-Turing估计法是很多平滑技术的核心
        - 其基本思路是：对于任何 一个出现r次的n元语法，都假设它出现了`r*`次
        - Good-Turing方法不能直接用于估计`n_r＝0`的n-gram概率，其中，`n_r`是训练语料中恰好出现r次的n元语法的数目
        - Good-Turing方法不能实现高阶模型与低阶模型的结合，而高低阶模型的结合通常是获得较好的平滑效果所必须的
    - Katz平滑法
        - Katz平滑方法通过加入高阶模型与低阶模型的结合，扩展了Good-Turing估计方法。
        - 所有具有非零计数r的二元语法都根据折扣率`d_r`被减值了
        - Katz平滑方法属于后备（back-off）平滑方法。这种方法的中心思想是，当某一事件在样本中出现的频率大于k时，运用最大似然估计经过 减值来估计其概率。当某一事件的频率小于k时，使用低阶
        的语法模型作为代替高阶语法模型的后备，而这种代替必须受归一化因子α的作 用。对于这种方法的另一种解释是，根据低阶的语法模型分配由于减值而节省下来的剩余概率给未见事件，这比将剩余概率平均分配给未见事件要合理
    - Jelinek-Mercer平滑方法
        - 因为冠词THE要比单词THOU出现的频率高得多。为了利用这种情况， 一种处理办法是在二元语法模型中加入一个一元模型。
        - 一般来讲，使用低阶的n元模型向高阶n元模型插值是有效的，因为当没有足够的语料估计高阶模型的概率时，低阶模型往往可以提供有用 的信息。
        - 第n阶平滑模型可以递归地定义为n阶最大似然估 计模型和n-1阶平滑模型之间的线性插值
    - Witten-Bell平滑方法
        - 它可以认为是Jelinek-Mercer平滑算法的一个实例
        - n阶平滑模型被递归地定义为n阶最大似然模型和n-1阶平滑模型的线性插值
    - 绝对减值法
        - 绝对减值法（absolute discounting）类似于Jelinek-Mercer平滑算法，涉及高阶和低阶模型的插值问题
        - 通过从每个非零计数中减去一个固定值`D≤1`的方法来建立高阶分布
    - Kneser-Ney平滑方法
    - Church-Gale平滑方法
    - 贝叶斯平滑方法
    - 修正的Kneser-Ney平滑方法
 - 平滑方法的比较：
    - **不管训练语料规模多大，对于二元语法和三元语法而言，Kneser-Ney平滑方法和修正的Kneser-Ney平滑方法的效果都好于其他所有的平滑方法**。
    - 一般情况下，Katz平滑方法和Jelinek-Mercer平滑方法也有较好的表现，但与上述两者平滑方法相比稍有逊色。
    - 在稀疏数据的情况下，Jelinek-Mercer平滑方法优于Katz平滑方法，而在有大量数据的情况下，Katz平滑方法则优于Jelinek-Mercer平滑方法
    - 插值的绝对减值平滑方法和后备的Witten-Bell平滑方法的表现最差
    - 除了对于很小的数据集以外，插值的绝对减值平滑方法一般优于留存插值方法的Jelinek-Mercer平滑方法
    - 而Witten-Bell平滑方法则表现较差，对于较小的数据集，该方法比留存插值方法的Jelinek-Mercer平滑方法差得多
    - 对于大规模数据集而言，这Witten-Bell平滑方法和插值的绝对减值平滑方法都比留存插值方法的Jelinek-Mercer平滑方法优越得多，甚至可以与Katz平滑方法和Jelinek-Mercer平滑方法相匹敌
    - 平滑方法的相对性能与训练语料的规模、n元语法模型的阶数和训练语料本身有较大的关系，其效果可能会随着这些因素的不同而出现很大的变化
    - 下列因素对于平滑算法的性能有一定的影响：
        - 影响最大的因素是采用修正的后备分布
        - 绝对减值优于线性减值
        - 从性能上来看，对于较低的非零计数，插值模型大大地优于后备模型
        - 增加算法的自由参数，并在留存数据上优化这些参数，可以改进算法的性能

**语言模型的自适应方法：**
 - 经常遇到的问题：
    - **模型对跨领域的脆弱性：**在训练语言模型时所采用的语料往往来自多种不同的领域，这些综合性语料难以反映不同领域之间在语言使用规律上的差异，而**语言模型恰恰对于训练文本的类型、主题和风格等都十分敏感**
    - **独立性假设的无效性：**n元语言模型的独立性假设前提是一个文本中的当前词出现的概率只与它前面相邻的n-1个词相关，但**这种假设在很多情况下是明显不成立的**。
 - 香农实验表明，相对而言，人更容易运用特定领域的语言知识、常识和领域知识进行推理以提高语言模型的性能（预测文本的下一个成分）。因此，**为了提高语言模型对语料的领域、主题、类型等因素的适应性，提出了自适应语言模型**
 - 语言模型自适应方法：
    - 基于缓存的语言模型
        - 在文本中刚刚出现过的一些词在后边的句子中再次出现的可能性往往较大，比标准的n元语法模型预测的概率要大
        - 针对这种现象，cache-based自适应方法的基本思路是，语言模型通过n元语法的线性插值求得，插值系数λ可以通过EM算法求得
        - （正常的基于缓存的语言模型）常用的方法是，在缓存中保留前面的K个单词，每个词wi的概率（缓存概率）用其在缓存中出现的相对频率计算得出
        - （衰减的基于缓存的语言模型）缓存中每个词对当前词的影响随着与该词距离的增大呈指数级衰减，越是临近的词，对缓存概率的贡献越大
        - cache-based自适应方法减低了语言模型的困惑度，衰减的比正常的给予缓存的语言模型对降低语言模型的困惑度效果更好
        - 黄非等（1999）提出了利用特定领域中少量自适应语料，在原词表中通过**分离通用领域词汇和特定领域词汇，并自动检测词典外领域关键词实现词典自适应**，然后结合基于缓存的方法实现语言模型的自适应方法
        - 曲卫民等（2003）通过采用TF-IDF公式代替原有的简单频率统计法，**建立基于记忆的扩展二元模型，并采用权重过滤法以节省模型计算量**，实现了对基于缓存记忆的语言模型自适应方法的改进
        - 张俊林等 （2005）也对基于记忆的语言模型进行了扩展，利用汉语义类词典，**将与缓存中所保留词汇语义上相近或者相关的词汇也引入缓存**，在一定程度上提高了原有模型的性能
    - 基于混合方法的语言模型
        - 基于混合方法的自适应语言模型针对的问题是：
            - 由于大规模训练语料本身是异源的，来自不同领域的语料无论在主题方面，还是在风格方面，或者同时在这两方面都有一定的差异
            - 而测试语料一般是同源的，因此，为了获得最佳性能，语言模型必须适应各种不同类型的语料对其性能的影响。
        - 基本思想是：
            - 将语言模型划分成n个子模型M1，M2，…，Mn，整个语言模型的概率通过下面的线性插值公式计算得到
                ```
                p(w_i|w_{1}^{i-1}) = sum_{j=1}^n lambda_j p_{M_j}(w_i|w_{1}^{i-1})
                ```
            lambda属于0到1，之和为1，可以通过EM算法计算
        - 基于这种思想，该适应方法针对测试语料的实现过程包括下列步骤：（即有针对性的训练子语言模型）
            - 对训练语料按来源、主题或类型等（不妨按主题）聚类；
            - 在模型运行时识别测试语料的主题或主题的集合；
            - 确定适当的训练语料子集，并利用这些语料建立特定的语言模型；
            - 利用针对各个语料子集的特定语言模型和线性插值公式，获得整个语言模型。
    - 基于最大熵的语言模型
        - 上面两种思路都是分别建立各个子模型，然后将子模型的输出组合起来
        - 基于最大熵的语言模型采用不同的思路，**即通过结合不同信息源的信息构建一个语言模型。每个信息源提供一组关于模型参数的约束条件，在所有满足约束的模型中，选择熵最大的模型**
        - 例子：
            ```
            考虑两个语言模型M1和M2，假设M1是标准的二元模型，表示为f函数：
            p_{M_1}(w_i|w_{1}^{i-1}) = f(w_i, w_{i-1})
            
            M2是距离为2的二元模型（distance-2 bigram），定义为g函数
            p_{M_2}(w_i|w_{1}^{i-1}) = g(w_i, w_{i-2})
            
            可以用线性插值方法取这两个概率的平均值，用后备方法选择其中一个进行数据平滑。
            
            为了考虑**远距离的文本历史信息**，以弥补一般语言模型仅仅利用近距离历史的不足，采用**触发器对**作为信息承载成分的思想。
             - 如果一个词序列A与另一个词序列B密切相关，那么，A→B被看作一个触发器对，
             - 其中，A为触发器，B为被触发的序列。
             - 当A出现在一个文本中时，它触发B，从而引起B的概率估计发生变化。
             - 如果把B看作当前词，A为历史h中的某个特征，那么，可以将一个二值的触发器对A→B形式化为一种约束函数f_{A→B}
            ```
        - 由于最大熵模型能够较好地将来自不同信息源的模型结合起来，获得性能较好的语言模型,因此，有些学者研究将基于主题的语言模型 （topic-based LM）（主题条件约束）与n元语法模型
        相结合，用于对话语音识别、信息检索IR和隐含语义分析(LSA)
 - 语言模型的自适应方法是改进和提高语言模型性能的重要手段之一。其性能表现与语料本身的状况（领域、主题、风格等）以及选用的统计基元等密切相关


## 神经网络语言模型
**一般概念：**
 - 在统计语言模型中，一个关键的问题是估计`P(Wt|W_{1:(t−1)})`，即在时刻（或位置）t，给定历史信息`h_t = w_{1:(t−1)}`条件下，词汇表V中的每个词v_k出现的概率。
 这个问题可以转换为一个类别数为|V|的多类分类问题，估计的词汇表中第k个词出现的后验概率。
 - 这样，我们就可以**使用机器学习中的不同分类器来估计语言模型的条件概率**。这里我们关注基于神经网络模型的方法，这一类方法可以统称为神经网络语言模型（NNLM）
 
**神经网络语言模型:**
 - 神经网络语言模型可以分为三个部分：
    - 输入层
        - 将语言符号序列`w_{1:(t-1)}`输入到神经网络模型中（以词嵌入矩阵的形式，稠密实数值向量）
        - 词嵌入矩阵`M ∈ R^{d1×|V|}`中，第k列向量`k ∈ R^{d1}`表示词汇表中第k个词对应的稠密向量
        - 通过直接映射，得到历史信息`w_{1:(t-1)}`每个词对应的向量表示`v_{w1},···,v_{w_{t−1}}`
    - 隐藏层
        - 隐藏层可以是不同类型的网络，前馈神经网络和循环神经网络，其输入为词向量`v_{w1},···,v_{w_{t−1}}`，输出为一个可以表示历史信息的向量`h_t`
        - 常见的网络类型：
            - 简单平均
                ```
                h_t = sum_{1}^{t-1} C_i v_{w_i}
                
                其中，C_i为每个词的权重。权重可以和位置相关，也可以和位置无关。位置无关的权重可以设置为C_i = 1/(t−1)
                ```
            - 前馈神经网络
                - 前馈神经网络要求输入的大小是固定的。
                - 因此，和n元模型类似，假设历史信息只包含前面n−1个词。首先将这n−1个的词向量拼接为一个维度为`d1 × (n−1)`的向量`x_t`
                - 然后将`x_t`输入到由多层前馈神经网络构成的隐藏层，最后一层隐藏层的输出`h_t`
                - 前馈网络的结构可以任意设置：
                    - 比如，只含一层隐藏层的模型为`h_t = tanh(W x_t + c)`
                    - 也可以包含跳层连接`h_t = x_t ⊕ tanh(W x_t + c)`
            - 循环神经网络
                - 和前馈神经网络不同，循环神经网络可以接受变长的输入序列，依次接受输入`v_{w1},···,v_{w_{t−1}}`，得到t时刻的隐藏状态`h_t`
        - 前馈网络语言模型和循环网络语言模型的不同之处在于**循环神经网络利用隐藏状态来记录以前所有时刻的信息，而前馈神经网络只能接受前(n-1)个时刻的信息。
    - 输出层
        - 输出层为大小为`|V|`，其接受的输入为历史信息的向量表示`h_t ∈ R^{d2}`，输出为**词汇表中每个词的后验概率**。
        - 在神经网络语言模型中，一般使用`softmax`分类器
            ```
            y_t = softmax(O h_t + b)
            
            输出向量y_t ∈ R^{|V|}为一个概率分布，
            O ∈ R^{|V|×d2}是最后一层隐藏层到输出层直接的权重。
            O也叫做**输出词嵌入矩阵**，矩阵中每一行也可以看作是一个词向量。
            ```
 - 训练
 - 大词汇表上`softmax`计算的改进
 - 基于采样方法的梯度近似估计

****




**GloVe：**



## word2vec数学原理
是一个开源工具包，用来获取词的向量


## Beam Search算法


## 序列生成算法


## 一些问题
**线性插值法和Attention机制：**


